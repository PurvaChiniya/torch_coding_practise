{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b650caed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.5740, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Implement L2-regularized logistic regression trained with minibatch gradient descent.\n",
    "# lets say we have some data \n",
    "import torch \n",
    "import torch.nn as nn \n",
    "torch.manual_seed(42)\n",
    "n_samples = 100 \n",
    "features = 64 \n",
    "X = torch.randn(n_samples, features)\n",
    "y = torch.randint(0,2, size=(n_samples, ),dtype = torch.int64)\n",
    "in_features = features\n",
    "out_features = 2 \n",
    "W = torch.randn(in_features, out_features , requires_grad=True)\n",
    "b = torch.randn(out_features, requires_grad=True)\n",
    "l2_lambda = 1e-2  # true L2 strength (on weights only)\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "# now we want to train a logistic regression with L2 norm and with minibatch gradient descent \n",
    "def fit_logistic_regression(X,y):\n",
    "    logits = torch.matmul(X,W)+ b \n",
    "    # we can compute the loss here as well \n",
    "    loss_total = ce_loss(logits, y) +  0.5*l2_lambda*(W**2).sum()\n",
    "    return logits, loss_total\n",
    "n_epochs = 10 \n",
    "lr = 0.01\n",
    "mini_batch = 20\n",
    "for _ in range(n_epochs):\n",
    "    for i in range(0,len(X),mini_batch ):\n",
    "        X_batch = X[i:i+mini_batch]\n",
    "        y_batch = y[i:i+mini_batch]\n",
    "        _, loss = fit_logistic_regression(X_batch, y_batch)\n",
    "        loss.backward()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            W-= lr*W.grad\n",
    "            b-= lr*b.grad\n",
    "        W.grad = None\n",
    "        b.grad = None\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e16bd9c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.5306, grad_fn=<AddBackward0>)\n",
      "tensor(31.7651, grad_fn=<AddBackward0>)\n",
      "tensor(19.3271, grad_fn=<AddBackward0>)\n",
      "tensor(16.7114, grad_fn=<AddBackward0>)\n",
      "tensor(13.6852, grad_fn=<AddBackward0>)\n",
      "tensor(6.4430, grad_fn=<AddBackward0>)\n",
      "tensor(3.8677, grad_fn=<AddBackward0>)\n",
      "tensor(2.1974, grad_fn=<AddBackward0>)\n",
      "tensor(5.1975, grad_fn=<AddBackward0>)\n",
      "tensor(10.7520, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "####Ques. 2 \n",
    "'''Input: X shape (N, D), labels y shape (N,) in {0,1}.\n",
    "Hidden layer: 32 units, ReLU activation.\n",
    "Output layer: 1 unit, sigmoid activation.\n",
    "Loss: binary cross-entropy.\n",
    "Optimizer: plain SGD, manually updating parameters.\n",
    "Regularization: L2 penalty on all weights (not biases).\n",
    "Mini-batch training, obviously.\n",
    "Print loss every few epochs. '''\n",
    "N_samples = 1000\n",
    "D = 64\n",
    "X = torch.randn(N_samples,D)\n",
    "y = torch.randint(0, 2, size= (N_samples , ), dtype=torch.float32)\n",
    "W1 = torch.randn(D, 32, requires_grad=True)\n",
    "b1 = torch.randn(32 ,requires_grad=True)\n",
    "relu = nn.ReLU()\n",
    "W2 =  torch.randn(32, 1,  requires_grad=True)\n",
    "b2 =  torch.randn( 1,  requires_grad=True)\n",
    "ce_loss = nn.BCEWithLogitsLoss()\n",
    "lambda_2 = 0.01\n",
    "lambda_1 = 0.02\n",
    "def forward(X, y):\n",
    "    y1 = X@W1+b1 \n",
    "    y1_relu = relu(y1)\n",
    "    y2 = y1_relu@W2 +b2 \n",
    "    y2 = y2.squeeze(1)\n",
    "    #y2_sigmoid = 1/(1+torch.exp(-y2))\n",
    "    return y2\n",
    "\n",
    "n_epochs = 10\n",
    "mini_batch = 8 \n",
    "lr = 0.0001\n",
    "for _ in range(n_epochs):\n",
    "    for i in range(0, N_samples, mini_batch):\n",
    "        y_batch = y[i:i+mini_batch]\n",
    "        X_batch = X[i:i+mini_batch]\n",
    "        y_logits  = forward(X_batch, y_batch )\n",
    "        loss = ce_loss(y_logits, y_batch) + lambda_1*0.5* (W1**2).sum() + lambda_2*0.5* (W2**2).sum()\n",
    "        loss.backward()\n",
    "        with torch.no_grad():\n",
    "            W1 -= lr*W1.grad\n",
    "            b1 -= lr*b1.grad\n",
    "            W2 -= lr*W2.grad\n",
    "            b2 -= lr*b2.grad\n",
    "    print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "15348627",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([700, 3])\n",
      "torch.Size([150, 3])\n",
      "torch.Size([150, 3])\n",
      "Epoch 2, Loss: 13.1744\n",
      "Epoch 2, Loss: 13.1910\n",
      "Epoch 2, Loss: 13.1722\n",
      "Epoch 2, Loss: 13.1712\n",
      "Epoch 2, Loss: 13.1738\n",
      "Epoch 2, Loss: 13.1747\n",
      "Epoch 2, Loss: 13.1500\n",
      "Epoch 4, Loss: 13.1185\n",
      "Epoch 4, Loss: 13.1293\n",
      "Epoch 4, Loss: 13.1164\n",
      "Epoch 4, Loss: 13.1157\n",
      "Epoch 4, Loss: 13.1195\n",
      "Epoch 4, Loss: 13.1201\n",
      "Epoch 4, Loss: 13.1082\n",
      "Best epoch: 4, Best val MSE: 13.1055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pchiniya/Desktop/hoverboard_workspace/mine/.conda/lib/python3.11/site-packages/torch/nn/modules/loss.py:616: UserWarning: Using a target size (torch.Size([100, 1000])) that is different to the input size (torch.Size([100, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "/Users/pchiniya/Desktop/hoverboard_workspace/mine/.conda/lib/python3.11/site-packages/torch/nn/modules/loss.py:616: UserWarning: Using a target size (torch.Size([150, 1000])) that is different to the input size (torch.Size([150, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "N = 1000\n",
    "torch.manual_seed(42)\n",
    "X = torch.randn(N, 3) # N, 3 \n",
    "noise = 0.1*torch.randn(N,1) \n",
    "#y = 3x1 - 2x2 + 0.5x3^2 + noise\n",
    "Y = 3*X[:,0] - 2*X[:,1] + 0.5*X[:,2]**2 +noise  # N, 1, shape \n",
    "class Model(nn.Module):\n",
    "    def __init__(self ):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(nn.Linear(3, 32), nn.ReLU(), nn.Linear(32, 16), nn.ReLU(), nn.Linear(16,1))\n",
    "        self.loss = torch.nn.MSELoss()\n",
    "        \n",
    "    def forward(self, x, y ):\n",
    "        # shape  of x is (N, 3 )\n",
    "        logits = self.model(x)\n",
    "        # shape (N,1 )\n",
    "        loss = self.loss(logits, y ) # shape is same as y and mean of that scalar output \n",
    "        return logits, loss \n",
    "\n",
    "n_train = int(0.7*N)\n",
    "n_val= int(0.85*N)\n",
    "x_train, y_train= X[:n_train] ,  Y[:n_train]\n",
    "x_test, y_test = X[n_train:n_val] ,  Y[n_train:n_val]\n",
    "x_val , y_val = X[n_val:] ,  Y[n_val:]\n",
    "print(x_train.shape)\n",
    "print(x_val.shape)\n",
    "print(x_test.shape)\n",
    "n_epochs = 5\n",
    "mini_batch=  100\n",
    "model = Model()\n",
    "best_val = float(\"inf\")\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr= 0.001, weight_decay=0.01 )\n",
    "train_loss_total = 0 \n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0 \n",
    "    for i in range(0, n_train,mini_batch ):\n",
    "        x_batch, y_batch = x_train[i:i+mini_batch],  y_train[i:i+mini_batch]\n",
    "        batch_logits, batch_loss = model(x_batch, y_batch)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        batch_loss.backward()\n",
    "        optimizer.step()\n",
    "        if (epoch + 1) % 2 == 0:\n",
    "            print(f\"Epoch {epoch+1}, Loss: {batch_loss.item():.4f}\")\n",
    "        train_loss += batch_loss.item()*x_batch.size(0)\n",
    "    train_loss_total += train_loss\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_preds, val_loss  = model(x_val, y_val)\n",
    "    improved = val_loss.item() < best_val - 1e-7\n",
    "    if improved:\n",
    "        best_val = val_loss\n",
    "        best_epoch = epoch\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "    else:\n",
    "        break \n",
    "print(f\"Best epoch: {best_epoch}, Best val MSE: {best_val:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "93b2bb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDropout(nn.Module):\n",
    "    def __init__(self,p):\n",
    "        super().__init__()\n",
    "        self.p = p \n",
    "        self.scale = 1/(1-self.p)\n",
    "        self.mask = None\n",
    "        \n",
    "    def forward(self,x):\n",
    "        if  not self.training: \n",
    "            return x \n",
    "    \n",
    "        mask = (torch.rand_like(x)>self.p)\n",
    "        return mask*x*self.scale\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "db6b9979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 2, 5])\n",
      "torch.Size([2, 10])\n",
      "torch.Size([2, 10])\n",
      "torch.Size([2, 10])\n",
      "torch.Size([2, 10])\n",
      "torch.Size([2, 10])\n",
      "torch.Size([2, 10])\n",
      "torch.Size([2, 10])\n",
      "torch.Size([2, 10])\n",
      "torch.Size([2, 10])\n",
      "torch.Size([2, 10])\n"
     ]
    }
   ],
   "source": [
    "class MyRNNCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.input_size =input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        # (sequence_length, batch_size, input_size)\n",
    "        self.hidden_to_hidden = nn.Linear(hidden_size, hidden_size)\n",
    "        self.input_to_hidden = nn.Linear(input_size, hidden_size)\n",
    "        self.initial_hidden = torch.randn(batch_size, hidden_size)\n",
    "        self.tanh = torch.nn.Tanh()\n",
    "    def forward(self, x,h_prev ):\n",
    "        \n",
    "        hidden_out = self.hidden_to_hidden(h_prev)\n",
    "        x_out = self.input_to_hidden(x)\n",
    "        \n",
    "        return  self.tanh(hidden_out + x_out )\n",
    "seq_len = 10 \n",
    "batch_size = 2\n",
    "input_size = 5 \n",
    "hidden_size = 10 \n",
    "x = torch.randn(seq_len, batch_size, input_size)\n",
    "print(x.shape)\n",
    "rnn = MyRNNCell(input_size,hidden_size )\n",
    "h_prev = torch.randn(batch_size, hidden_size)\n",
    "outputs = []\n",
    "for i in range(seq_len):\n",
    "    x_t = x[i, :, :]\n",
    "    print(h_prev.shape)\n",
    "    h_prev = rnn(x_t, h_prev )\n",
    "    outputs.append(h_prev)\n",
    "outputs = torch.stack(outputs, dim=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162ea7c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, num_heads, d_model):\n",
    "        super().__init__()\n",
    "        # we need ot deine the QKV matrices we project to \n",
    "        self.num_heads = num_heads\n",
    "        self.dk = d_model/ num_heads\n",
    "        self.d_model = d_model \n",
    "        self.WQ = nn.Linear(self.d_model, self.d_model) # projections are applied before any head splits \n",
    "        self.WK = nn.Linear(self.d_model, self.d_model) # projections are applied before any head splits \n",
    "        self.WV = nn.Linear(self.d_model, self.d_model) # projections are applied before any head splits \n",
    "        self.WO = nn.Linear(self.d_model, self.d_model) # projections are applied before any head splits \n",
    "    def forward(self, q, k , v):\n",
    "        query = self.WQ(q)\n",
    "        key = self.WK(k)\n",
    "        value = self.WV(v) # batch_size, seq_len, d_model \n",
    "        # now we can split the head and call the attention \n",
    "        batch_size = q.shape[0]\n",
    "        # split the headds \n",
    "        query = torch.view(batch_size, -1, self.num_heads, self.dk).transpose(1,2)\n",
    "        value = torch.view(batch_size, -1, self.num_heads, self.dk).transpose(1,2)\n",
    "        key = torch.view(batch_size, -1, self.num_heads, self.dk).transpose(1,2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7fb8044",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def masked_batch_cosine(a: torch.Tensor, b: torch.Tensor, mask: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    a: (B, N, D), b: (B, M, D), mask: (B, N, M) boolean, where True means \"include this pair\".\n",
    "    Returns: (B,) mean cosine similarity over masked pairs per batch.\n",
    "    If a batch has 0 valid pairs, return 0 for that batch (not NaN), with proper gradient behavior.\n",
    "    \"\"\"\n",
    "    B= 10 \n",
    "    N = 10; M = 8 \n",
    "    D = 76\n",
    "    a = torch.tensor(B, N, D)\n",
    "    b = torch.tensor(B,M,D)\n",
    "    mask = torch.randint(0, 2, size = (B, N, M ))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "20eaf5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "B= 10 \n",
    "N = 10; M = 8 \n",
    "D = 76\n",
    "a = torch.randn(B, N, D)\n",
    "b = torch.randn(B,M,D)\n",
    "mask = torch.randint(0, 2, size = (B, N, M ))\n",
    "matrix_multiplication = torch.matmul(a, b.transpose(1, 2))\n",
    "a_mod = torch.norm(a)\n",
    "b_mod = torch.norm(b)\n",
    "matrix_multiplication = matrix_multiplication /(a_mod*b_mod)\n",
    "y = matrix_multiplication[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16f43a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10, 8])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "538a75ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_knn(X_train: torch.Tensor, y_train: torch.Tensor, x_test: torch.Tensor, k: int) -> int:\n",
    "    \"\"\"\n",
    "    Predicts the class label for a single test point x_test.\n",
    "\n",
    "    Args:\n",
    "        X_train: The training data features (shape: [num_train_samples, num_features])\n",
    "        y_train: The training data labels (shape: [num_train_samples])\n",
    "        x_test: The single test point to classify (shape: [num_features])\n",
    "        k: The number of neighbors to consider\n",
    "    \n",
    "    Returns:\n",
    "        The predicted class label (an integer)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "1887fc37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7821, -1.3115],\n",
      "        [ 0.3817,  0.6157]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 1],\n",
       "        [0, 0, 1]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = torch.tensor([\n",
    "    [1.0, 1.0],  # Sample 0\n",
    "    [1.5, 2.0],  # Sample 1\n",
    "    [5.0, 5.0],  # Sample 2\n",
    "    [4.5, 5.5]   # Sample 3\n",
    "]) # 4 x 2 \n",
    "y_train = torch.tensor([0, 0, 1, 1])\n",
    "#x_test = torch.tensor([1.2, 1.3]) # 1 x 2  \n",
    "N = 2\n",
    "x_test = torch.randn(N, 2)#(num_test_samples, num_features)\n",
    "print(x_test)\n",
    "k = 3\n",
    "# k nearest neghbors, the class of new poitn is majority of these 3 neighbors \n",
    "# calculate distance to all the X_Train poitns \n",
    "#distances = torch.sqrt(torch.sum((X_train - x_test)**2, dim=1))\n",
    "\n",
    "# distance = X_train - x_test\n",
    "# top_k_values, top_k_indices = torch.topk(distances, k = k,largest = False )\n",
    "# label, _ = torch.mode(y_train[top_k_indices])\n",
    "# print(label)\n",
    "x_test  =  torch.unsqueeze(x_test, 1)\n",
    "\n",
    "distances  = torch.sqrt(torch.sum((X_train - x_test)**2, dim = 2)) # num test, num_train, featuers \n",
    "top_k_values, top_k_indices = torch.topk(distances, k = k,largest = False )\n",
    "y_train[top_k_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "4634ba45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = torch.tensor([\n",
    "    [1.0, 1.0],  # 0\n",
    "    [1.5, 2.0],  # 0\n",
    "    [5.0, 5.0],  # 1\n",
    "    [4.5, 5.5],  # 1\n",
    "    [1.1, 1.1]   # 0\n",
    "])\n",
    "y_train = torch.tensor([0, 0, 1, 1, 0])\n",
    "\n",
    "# Batch of 2 test points\n",
    "X_test_batch = torch.tensor([\n",
    "    [1.2, 1.3],  # Should be 0\n",
    "    [4.9, 5.1]   # Should be 1\n",
    "])\n",
    "k = 3\n",
    "# to broadcast the distances to be xtest, xtrain, features shape \n",
    "X_test_batch = X_test_batch.unsqueeze(1)\n",
    "distances = torch.sum((X_train -X_test_batch)**2, dim = 2 ) # sum along the features dimentions \n",
    "top_k_distances, top_k_indices = torch.topk(distances, k = k ,largest =False)\n",
    "k_nearest_labels = y_train[top_k_indices]\n",
    "predicted_labels = torch.mode(k_nearest_labels, dim=1).values # dimentsion along the x train \n",
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "bab48a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# weightted KNN\n",
    "num_classes = y_train.max().item() + 1\n",
    "# only instad of taking mode now, you mutlipy by weights and take the final one \n",
    "weights = 1.0 / (top_k_distances + 1e-6)\n",
    "num_test = X_test_batch.shape[0]\n",
    "scores = torch.zeros(num_test, num_classes)\n",
    "scores.scatter_add_(dim = 1, index=k_nearest_labels, src=weights)\n",
    "predicted_labels = torch.argmax(scores, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "97de74a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "821a648b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K means \n",
    "X= torch.randn(100, 3)\n",
    "k= 4 \n",
    "max_iters = 10 \n",
    "\n",
    "centroids = X[torch.randperm(X.shape[0])[:k]]\n",
    "X = X.unsqueeze(1) # k, 3 goes to k, 1, 3 \n",
    "# find closest points to this centroids \n",
    "centroids.shape\n",
    "for _ in range(max_iters):\n",
    "    old_centroids = centroids.clone()\n",
    "    distances  = torch.sqrt(torch.sum((X- centroids)**2 , dim = 2))# here we broadcast to k , N, 3 # we sum across last dim to get N, k \n",
    "    # for each N poitns we have distance to each  K centroids\n",
    "    # assign each poitn to a centroids \n",
    "    labels = torch.argmin(distances, dim = 1 )\n",
    "    # shape i sN \n",
    "    # these are the new labels \n",
    "    # new centroids are the cneters of these poitns \n",
    "    for i in range(k):\n",
    "        centroids[i] = torch.mean(X[labels==i]) # N, 3 \n",
    "    if torch.allclose(old_centroids, centroids):\n",
    "        break \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "639177fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2544,  0.2544,  0.2544],\n",
       "        [ 0.8059,  0.8059,  0.8059],\n",
       "        [-0.5770, -0.5770, -0.5770],\n",
       "        [ 2.0863,  0.2419, -0.8668]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "3d0be5c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: numpy>=1.23.2 in ./.conda/lib/python3.11/site-packages (from pandas) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/pchiniya/.local/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/pchiniya/.local/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.3-cp311-cp311-macosx_11_0_arm64.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [pandas]2m2/3\u001b[0m [pandas]\n",
      "\u001b[1A\u001b[2KSuccessfully installed pandas-2.3.3 pytz-2025.2 tzdata-2025.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "32c6f897",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'some_path.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[154]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, index):\n\u001b[32m     14\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.features[index], \u001b[38;5;28mself\u001b[39m.labels[index])\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m my_dataset = \u001b[43mMyCSVDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msome_path.csv\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m num_epochs = \u001b[32m10\u001b[39m \n\u001b[32m     17\u001b[39m train_loader = DataLoader(dataset=my_dataset, batch_size=\u001b[32m32\u001b[39m, shuffle=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[154]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mMyCSVDataset.__init__\u001b[39m\u001b[34m(self, csv_file_path)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, csv_file_path):\n\u001b[32m      7\u001b[39m     \u001b[38;5;28msuper\u001b[39m().\u001b[34m__init__\u001b[39m()\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     df = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_file_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mself\u001b[39m.features = torch.tensor(df.iloc[:, \u001b[32m0\u001b[39m:\u001b[32m4\u001b[39m].values, dtype = torch.float32)\n\u001b[32m     10\u001b[39m     \u001b[38;5;28mself\u001b[39m.labels =  torch.tensor(df.iloc[:, \u001b[32m4\u001b[39m].values,dtype=torch.long)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hoverboard_workspace/mine/.conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1013\u001b[39m kwds_defaults = _refine_defaults_read(\n\u001b[32m   1014\u001b[39m     dialect,\n\u001b[32m   1015\u001b[39m     delimiter,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1022\u001b[39m     dtype_backend=dtype_backend,\n\u001b[32m   1023\u001b[39m )\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hoverboard_workspace/mine/.conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    617\u001b[39m _validate_names(kwds.get(\u001b[33m\"\u001b[39m\u001b[33mnames\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[32m    623\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hoverboard_workspace/mine/.conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1617\u001b[39m     \u001b[38;5;28mself\u001b[39m.options[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m] = kwds[\u001b[33m\"\u001b[39m\u001b[33mhas_index_names\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hoverboard_workspace/mine/.conda/lib/python3.11/site-packages/pandas/io/parsers/readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1878\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1891\u001b[39m f = \u001b[38;5;28mself\u001b[39m.handles.handle\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hoverboard_workspace/mine/.conda/lib/python3.11/site-packages/pandas/io/common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    868\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m    869\u001b[39m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[32m    870\u001b[39m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[32m    871\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m         handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[32m    882\u001b[39m         handle = \u001b[38;5;28mopen\u001b[39m(handle, ioargs.mode)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'some_path.csv'"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import pandas as pd \n",
    "epochs_no_improve = 0 \n",
    "from torch.utils.data import DataLoader\n",
    "class MyCSVDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, csv_file_path):\n",
    "        super().__init__()\n",
    "        df = pd.read_csv(csv_file_path)\n",
    "        self.features = torch.tensor(df.iloc[:, 0:4].values, dtype = torch.float32)\n",
    "        self.labels =  torch.tensor(df.iloc[:, 4].values,dtype=torch.long)\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    def __getitem__(self, index):\n",
    "        return (self.features[index], self.labels[index])\n",
    "my_dataset = MyCSVDataset(\"some_path.csv\")\n",
    "num_epochs = 10 \n",
    "train_loader = DataLoader(dataset=my_dataset, batch_size=32, shuffle=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "ce_loss = nn.CrossEntropyLoss()\n",
    "val_loader = DataLoader(dataset=my_dataset, batch_size=8, shuffle=True)\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_train_loss = 0 \n",
    "    # train_loader handles all the batching and shuffling automatically!\n",
    "    for features_batch, labels_batch in train_loader:\n",
    "        preds = model(features_batch) \n",
    "        loss = ce_loss(preds, labels_batch)\n",
    "        total_train_loss += loss.item()\n",
    "        optimizer.zero_grad(set_to_none=False) # true is faster \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    model.eval() # FIX: Put model in evaluation mode\n",
    "    total_val_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for val_features, val_labels in val_loader:\n",
    "        \n",
    "            val_preds =  model(val_features)\n",
    "            val_loss = ce_loss(val_preds, val_labels)\n",
    "            total_val_loss+= (val_loss.item())\n",
    "    avg_val_loss  = total_val_loss/len(val_loader)\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        epochs_no_improve = 0 \n",
    "    else:\n",
    "        epochs_no_improve+=1\n",
    "    if epochs_no_improve >= 4:\n",
    "        print(f\"Early stopping at epoch {epoch+1} as val loss did not improve.\")\n",
    "        break # Stop training\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e319d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kernel_size and stride.\n",
    "import torch.nn as nn \n",
    "batch_size = 8 \n",
    "channels = 3 \n",
    "in_height = 20 \n",
    "in_width = 40\n",
    "X = torch.randn(batch_size, channels, in_height, in_width)\n",
    "class MyMaxPool2d(nn.Module):\n",
    "    def __init__(self, kernel_size, stride):\n",
    "        super().__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride \n",
    "    def forward(self, X):\n",
    "        (batch_size, channels, in_height, in_width) = X.shape\n",
    "        out_height = (in_height - self.kernel_size)// self.stide +1 \n",
    "        out_width  = (in_width - self.kernel_size)// self.stide +1 \n",
    "        output = torch.zeros(batch_size, channels, out_height, out_width, \n",
    "                             dtype=x.dtype, device=x.device)\n",
    "        for b in range(batch_size):\n",
    "            for c in range(channels):\n",
    "                for h_out in range(out_height):\n",
    "                    for w_out in range(out_width):\n",
    "                        h_start = h_out * self.stride\n",
    "                        w_start = w_out * self.stride \n",
    "                        h_end = h_start+  self.kernel_size\n",
    "                        w_end  = w_start+  self.kernel_size\n",
    "                        window = x[b, c, h_start:h_end, w_start:w_end]\n",
    "                        max_val = torch.max(window)\n",
    "                        output[b, c, h_out, w_out] = max_val\n",
    "        return output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed107f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
