{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2be3a48d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe Complexity DifferenceWithout Cache:\\n To generate token $N$, we process $N$ tokens. Total for a sequence is $\\\\sum_{i=1}^{N} i \\x07pprox O(N^2)$.\\n With Cache: To generate token $N$, we process $1$ token (attending to $N-1$ cached keys).\\n   Total for a sequence is $\\\\sum_{i=1}^{N} 1 = O(N)$.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LLM inference and optimisations\n",
    "# KV cache , optimises the token generation pipeline ofro N tokens from O(N3) summation O(n2) for each token = n3 to \n",
    "# O(n) for each n tokens = O(n2) complexity \n",
    "import torch \n",
    "import math \n",
    "import torch.nn as nn \n",
    "def scaled_dot_product_attention(Q, K, V, mask=  None):\n",
    "    # Q,K,V: (B, H, T, D)\n",
    "    scores = torch.matmul(Q, K.transpose(-1, -2)) / math.sqrt(Q.size(-1))  # (B,H,T,Tk)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, float(\"-inf\"))\n",
    "    probs = torch.softmax(scores, dim=-1)\n",
    "    out = torch.matmul(probs, V)  # (B,H,T,D)\n",
    "    return out, probs  # return both; caller can ignore probs if not needed\n",
    "'''\n",
    "The Complexity DifferenceWithout Cache:\n",
    " To generate token $N$, we process $N$ tokens. Total for a sequence is $\\sum_{i=1}^{N} i \\approx O(N^2)$.\n",
    " With Cache: To generate token $N$, we process $1$ token (attending to $N-1$ cached keys).\n",
    "   Total for a sequence is $\\sum_{i=1}^{N} 1 = O(N)$.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d71cec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, use_kv_cache= False):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.dk = d_model // num_heads\n",
    "        self.use_kv_cache = use_kv_cache\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        # q groupd \n",
    "\n",
    "    def forward(self, x, kv_cache = None):\n",
    "        # B, 10 , d \n",
    "        # B 11, d \n",
    "        # B ,1 , d \n",
    "        q = self.w_q(x) # (B, T, d_model)\n",
    "        k = self.w_k(x) # (B, T, d_model)\n",
    "        v = self.w_v(x) # (B, T, d_model)\n",
    "        # now split into heads \n",
    "        B, T, _ = q.shape\n",
    "\n",
    "        q = q.view(B, T, self.num_heads, self.dk).transpose(1, 2) # (B, H, T, dk)\n",
    "        k_new = k.view(B, T, self.num_heads, self.dk).transpose(1, 2) # (B, H, T, dk)\n",
    "        v_new  = v.view(B, T, self.num_heads, self.dk).transpose(1, 2) # (B, H, T, dk)\n",
    "        # B, 1, d  , t+1,d \n",
    "        # B, 1, d , t+1, d\n",
    "        # now we can do scaled dot product attention\n",
    "        # lets say we are not using cache here \n",
    "        # if you are using the kv cache q, kv, becomes B, h, 1, dk because x is a single token\n",
    "        # so instaed ofmultiplying b, n, d with dx d we now multiply b, 1, d \n",
    "        if self.use_kv_cache:\n",
    "            if kv_cache is not None:\n",
    "                k, v = kv_cache                              # (B, H, T_prev, Dh)\n",
    "                k = torch.cat([k, k_new], dim=2)              # (B, H, T_prev+1, Dh)\n",
    "                v = torch.cat([v, v_new], dim=2)              # (B, H, T_prev+1, Dh)\n",
    "            else:\n",
    "                k, v = k_new, v_new\n",
    "            new_cache = (k, v)\n",
    "            mask = None\n",
    "        else:\n",
    "            k, v = k_new, v_new\n",
    "            new_cache = None\n",
    "            # causal mask (lower-triangular)\n",
    "            mask = torch.tril(torch.ones(T, T, device=x.device)).view(1, 1, T, T)  # (1,1,T,T)\n",
    "\n",
    "        out, probs  = scaled_dot_product_attention(q, k, v) # (B, H, T, dk)\n",
    "        out = out.transpose(1, 2).contiguous().view(B, T, self.d_model)  # (B, T, d_model)\n",
    "\n",
    "        out = self.w_o(out) \n",
    "        return out , kv_cache\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "25bc33f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- WITHOUT KV CACHE ---\n",
      "\n",
      "Step 1: input torch.Size([1, 1, 64])\n",
      "\n",
      "Step 2: input torch.Size([1, 2, 64])\n",
      "\n",
      "Step 3: input torch.Size([1, 3, 64])\n",
      "\n",
      "Step 4: input torch.Size([1, 4, 64])\n",
      "\n",
      "Step 5: input torch.Size([1, 5, 64])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "X = torch.rand(1, 1, 64)  # (B=1, seq=1, d_model=64)\n",
    "num_tokens = 5\n",
    "print(\"\\n--- WITHOUT KV CACHE ---\")\n",
    "attn_no = MultiheadAttention(64, 8, use_kv_cache=False)\n",
    "for t in range(1, num_tokens + 1):\n",
    "    x_in = X.repeat(1, t, 1)  # simulate full sequence each step\n",
    "    print(f\"\\nStep {t}: input {x_in.shape}\")\n",
    "    y, _ = attn_no(x_in)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "b5749d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- WITH KV CACHE ---\n",
      "\n",
      "Step 1: input torch.Size([1, 1, 64])\n",
      "\n",
      "Step 2: input torch.Size([1, 1, 64])\n",
      "\n",
      "Step 3: input torch.Size([1, 1, 64])\n",
      "\n",
      "Step 4: input torch.Size([1, 1, 64])\n",
      "\n",
      "Step 5: input torch.Size([1, 1, 64])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- WITH KV CACHE ---\")\n",
    "attn_cache = MultiheadAttention(64, 8, use_kv_cache=True)\n",
    "kv_cache = None\n",
    "for t in range(num_tokens):\n",
    "    x_in = X  # one token at a time\n",
    "    print(f\"\\nStep {t+1}: input {x_in.shape}\")\n",
    "    y, kv_cache = attn_cache(x_in, kv_cache)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3baa3d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "07800f82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original weights:\n",
      " tensor([[-0.5614,  0.7887,  0.4191,  1.0952],\n",
      "        [-0.6537,  1.5501, -1.4322,  0.3474]])\n",
      "\n",
      "Quantized (uint8):\n",
      " tensor([[ 74, 189, 158, 216],\n",
      "        [ 66, 255,   0, 152]], dtype=torch.uint8)\n",
      "\n",
      "Dequantized weights:\n",
      " tensor([[-0.5614,  0.7836,  0.4210,  1.0993],\n",
      "        [-0.6549,  1.5554, -1.4268,  0.3508]])\n",
      "\n",
      "FP32 output: tensor([[-0.5213,  0.6229]])\n",
      "INT8(dequantized) output: tensor([[-0.5218,  0.6176]])\n",
      "Max diff: 0.005379140377044678\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# random weights\n",
    "W = torch.randn(2, 4)\n",
    "x = torch.randn(1, 4)\n",
    "\n",
    "# --- quantization ---\n",
    "W_min, W_max = W.min(), W.max()\n",
    "qmin, qmax = 0, 255\n",
    "scale = (W_max - W_min) / (qmax - qmin)\n",
    "zero_point = int(qmin - W_min / scale)\n",
    "\n",
    "# convert to uint8\n",
    "W_q = torch.round(W / scale + zero_point).clamp(qmin, qmax).to(torch.uint8)\n",
    "\n",
    "# --- dequantization ---\n",
    "W_deq = scale * (W_q.float() - zero_point)\n",
    "\n",
    "# --- run inference ---\n",
    "y_fp32 = x @ W.T\n",
    "y_int8 = x @ W_deq.T\n",
    "\n",
    "print(\"Original weights:\\n\", W)\n",
    "print(\"\\nQuantized (uint8):\\n\", W_q)\n",
    "print(\"\\nDequantized weights:\\n\", W_deq)\n",
    "print(\"\\nFP32 output:\", y_fp32)\n",
    "print(\"INT8(dequantized) output:\", y_int8)\n",
    "print(\"Max diff:\", (y_fp32 - y_int8).abs().max().item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209c823b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
