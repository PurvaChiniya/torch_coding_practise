{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e44be6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Starter tensors\n",
    "a = torch.arange(24).reshape(2, 3, 4).float()\n",
    "b = torch.randn(2, 3, 4)\n",
    "\n",
    "# Tasks:\n",
    "# 1. Compute mean and standard deviation of `a` along last dim.\n",
    "# 2. Normalize `a` along dim=1 (so sum over dim=1 equals 1).\n",
    "# 3. Compute matrix multiplication between a[0] and b[0]^T (transpose b[0]).\n",
    "# 4. Flatten a into shape (3, 8) *without* using view(-1, 8).\n",
    "# 5. Permute a to shape (4, 2, 3) ‚Äî double-check with a.shape.\n",
    "mean_a = torch.mean(a, dim=-1)\n",
    "std_a = torch.var(a, dim= -1)\n",
    "\n",
    "second = torch.softmax(a, dim=1)\n",
    "\n",
    "third = torch.matmul(a[0], torch.transpose(b[0], -1, -2)) # output should be of size 3 3 \n",
    "# shape of a is 2, 3, 4 \n",
    "fourth = a.reshape(3,8)\n",
    "\n",
    "a = a.permute(2,0,1)\n",
    "\n",
    "# b[0] shape is 3, 4 transpose -1,and -2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1f7654d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.4919,  1.3355, -0.4124, -0.1862,  0.0107])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([-0.4919,  1.3355, -0.4124, -0.1862,  0.0107])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(5, 5)\n",
    "\n",
    "# Tasks:\n",
    "# 1. Replace all negative values with their absolute values.\n",
    "# 2. Create a mask of all values > mean(x), and set them to 0.\n",
    "# 3. Extract the diagonal elements using advanced indexing.\n",
    "# 4. Normalize each row so that its L2 norm = 1.\n",
    "# 5. Write a single-line expression to reverse the rows of x.\n",
    "# 1 \n",
    "mask = x<0\n",
    "y = torch.where(x<0, -x,x)\n",
    "\n",
    "x_mean = torch.mean(x)\n",
    "y = torch.where(x>x_mean, 0, x)\n",
    "\n",
    "# extract the diagonal \n",
    "y = torch.arange(0,5)\n",
    "\n",
    "diag = x[y,y]\n",
    "print(diag)\n",
    "# normalise each row that L2 norm is 1 \n",
    "#x2 sum is 1 \n",
    "a= torch.arange(x.shape[0] )\n",
    "mask = torch.zeros_like(x, dtype=torch.bool)\n",
    "mask[a, a] = True\n",
    "x[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "70dd4704",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1931, -0.1491, -0.6344, -0.6825,  0.2688],\n",
       "        [-0.2968,  0.6635, -0.4444, -0.4904, -0.1837],\n",
       "        [-0.7733,  0.3450, -0.1551,  0.2395,  0.4488],\n",
       "        [ 0.7648,  0.5879, -0.0982, -0.1344,  0.2045],\n",
       "        [-0.1166,  0.2184, -0.7496, -0.6138,  0.0062]])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "row_norm = torch.sqrt(torch.sum(x*x, dim=1, keepdim=True))\n",
    "x = x/row_norm \n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "2b6eb5af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2688, -0.6825, -0.6344, -0.1491, -0.1931],\n",
       "        [-0.1837, -0.4904, -0.4444,  0.6635, -0.2968],\n",
       "        [ 0.4488,  0.2395, -0.1551,  0.3450, -0.7733],\n",
       "        [ 0.2045, -0.1344, -0.0982,  0.5879,  0.7648],\n",
       "        [ 0.0062, -0.6138, -0.7496,  0.2184, -0.1166]])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.flip(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "6e47478f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n",
      "torch.Size([3, 1])\n",
      "torch.Size([3, 5])\n",
      "tensor([[1., 0., 0., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.]])\n",
      "tensor([0.2583, 0.3368, 0.2760])\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, 3, requires_grad=True)\n",
    "\n",
    "# Tasks:\n",
    "# 1. Compute y = (x ** 2).sum() and backprop to get dy/dx.\n",
    "# 2. Compute z = torch.exp(x) + torch.sin(x); find dz/dx.\n",
    "# 3. Create tensors a(3,4,5) and b(4,5) and add them via broadcasting.\n",
    "# 4. Manually compute softmax(x, dim=1) using tensor ops only (no torch.softmax).\n",
    "# 5. Implement cross entropy loss manually given logits and one-hot targets.\n",
    "y = torch.sum(x**2)\n",
    "y_grad = torch.autograd.grad(y, x)[0]\n",
    "print(y_grad.shape)\n",
    "\n",
    "z = torch.exp(x) + torch.sin(x)\n",
    "z_grad, = torch.autograd.grad(z, x, grad_outputs=torch.ones_like(z))\n",
    "\n",
    "\n",
    "# 3 \n",
    "a = torch.randn(3,4, 5)\n",
    "b = torch.randn(4,5)\n",
    "c = a+b\n",
    "# this work only if the last two are same, but no broadcasting if first two are smae\n",
    "#So yeah ‚Äî broadcasting only cares that the last dimensions align, not the first ones.\n",
    "c.shape\n",
    "\n",
    "# 4 \n",
    "x = torch.randn(3, 3, requires_grad=True)\n",
    "# make this nuemrically stable \n",
    "x_max = torch.max(x, dim= 1, keepdim= True).values\n",
    "x = x-x_max\n",
    "x_exp = torch.exp(x) # 3, 3\n",
    "x_sum = torch.sum(x_exp, dim =1, keepdim= True)\n",
    "soft_x = x_exp/ x_sum \n",
    "soft_x.shape\n",
    "\n",
    "# 5. Implement cross entropy loss manually given logits and one-hot targets.\n",
    "logits = torch.rand(3, 5)\n",
    "# 3 samples, 5 classes \n",
    "\n",
    "one_hot = torch.zeros_like(logits)\n",
    "a = torch.arange(3)\n",
    "a = a.unsqueeze(1)\n",
    "print(a.shape)\n",
    "print(one_hot.shape)\n",
    "one_hot.scatter_(1, a, 1 )\n",
    "print(one_hot)\n",
    "# CE loss \n",
    "CE_loss = - torch.mean(torch.log(torch.softmax(logits,dim= 1 ))*one_hot, dim = 1 )\n",
    "print(CE_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "d3813c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10])\n"
     ]
    }
   ],
   "source": [
    "# üß¨ Level 4 ‚Äì Spatial Thinking (Matrix Ops & Geometry)\n",
    "coords = torch.randn(10, 3)\n",
    "rot = torch.tensor([\n",
    "    [0.0, -1.0, 0.0],\n",
    "    [1.0,  0.0, 0.0],\n",
    "    [0.0,  0.0, 1.0]\n",
    "])\n",
    "\n",
    "# Tasks:\n",
    "# 1. Rotate all coords using batched matrix multiplication.\n",
    "# 2. Compute pairwise Euclidean distance between all points.\n",
    "# 3. Compute pairwise cosine similarity using broadcasting.\n",
    "# 4. Implement k-nearest neighbors (no loops).\n",
    "# 5. Center all points around origin, then rescale them to unit variance.\n",
    "\n",
    "\n",
    "y = torch.bmm(coords.unsqueeze(0), rot.unsqueeze(0)).squeeze(0)\n",
    "# 2. Compute pairwise Euclidean distance between all points.\n",
    "#y = coords[:, None, :] - coords[None, : , :]\n",
    "# coords: (N, D)\n",
    "x_norm = (coords ** 2).sum(dim=1, keepdim=True)   # (N, 1)\n",
    "# pairwise squared distances\n",
    "dists_sq = x_norm + x_norm.T - 2 * coords @ coords.T\n",
    "# clamp and sqrt for clean distances\n",
    "dists = torch.sqrt(torch.clamp(dists_sq, min=0.0))\n",
    "\n",
    "# 3 cosine similarity \n",
    "coords = coords / coords.norm(dim=1, keepdim=True)\n",
    "cos_sim = torch.matmul(coords, coords.transpose(-1,-2))\n",
    "print(cos_sim.shape)\n",
    "\n",
    "\n",
    "# k nearest neighbors for coords \n",
    "# for each coord top k nearest neighbors \n",
    "k = 3 \n",
    "x_norm = (coords ** 2).sum(dim=1, keepdim=True) \n",
    "dist = x_norm + torch.transpose(x_norm, -1, -2) - 2 * torch.matmul(coords, torch.transpose(coords, -1, -2))\n",
    "dist.shape\n",
    "# choose top k from these distances for each row \n",
    "# either fill the diagonal with inf or chooose k+1 because the coord itself is the first closest point in it \n",
    "dist.fill_diagonal_(float('inf'))                    # ignore self\n",
    "\n",
    "top_k_values, top_k_indices = torch.topk(dist, k, dim=1, largest= False) # for sorting \n",
    "# for each row choose the top k in column? \n",
    "\n",
    "\n",
    "## col5 \n",
    "x_mean = torch.mean(x, dim =0 , keepdim=True)\n",
    "x_new = x-x_mean\n",
    "x_std = torch.std(x, dim=0, keepdim=True)\n",
    "x_new= x_new/ x_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "2c1330fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ü§Ø Level 5 ‚Äì ‚ÄúWhy did I sign up for this?‚Äù (Pure Tensor Wizardry)\n",
    "# Build a mini 2-layer neural net *from scratch*:\n",
    "# Input: (batch_size, 5), Hidden: 4, Output: 2\n",
    "# Activation: ReLU\n",
    "# Loss: MSE between predicted and target\n",
    "\n",
    "batch_size = 8\n",
    "x = torch.randn(batch_size, 5)\n",
    "y = torch.randn(batch_size, 2)\n",
    "\n",
    "W1 = torch.randn(5, 4, requires_grad=True)\n",
    "b1 = torch.zeros(4, requires_grad=True)\n",
    "W2 = torch.randn(4, 2, requires_grad=True)\n",
    "b2 = torch.zeros(2, requires_grad=True)\n",
    "\n",
    "# Tasks:\n",
    "# 1. Forward pass using only tensor ops (no nn.Module).\n",
    "# 2. Compute loss = ((y_pred - y) ** 2).mean()\n",
    "# 3. Backprop manually using autograd.\n",
    "# 4. Update weights using gradient descent with lr=0.01.\n",
    "\n",
    "f = torch.relu(x@W1+b1)\n",
    "output = f@W2 +b2 \n",
    "#f.shape # should be 8 x 2 \n",
    "\n",
    "loss = torch.mean((output-y)**2, dim=(0,1))\n",
    "\n",
    "\n",
    "W1_grad,  = torch.autograd.grad(loss,W1, retain_graph=True)\n",
    "\n",
    "W2_grad,  = torch.autograd.grad(loss,W2,  retain_graph=True)\n",
    "b2_grad, = torch.autograd.grad( loss,b2,  retain_graph=True)\n",
    "b1_grad, = torch.autograd.grad( loss,b1,  retain_graph=True)\n",
    "\n",
    "\n",
    "lr = torch.tensor(0.01, dtype=torch.float16)\n",
    "with torch.no_grad():\n",
    "    b1 -= lr*b1_grad\n",
    "    b2 -= lr*b2_grad\n",
    "    W1 -= lr*W1_grad\n",
    "    W2 -= lr*W2_grad\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "83e56cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# batchnorm \n",
    "# X (batch_size, features)\n",
    "x = torch.randn(8, 5)\n",
    "\n",
    "x_mean = torch.mean(x, dim = 0, keepdim= True)\n",
    "x_var = torch.var(x, dim=0, keepdim=True)\n",
    "x = (x-x_mean)/ torch.sqrt(x_var+1e-15)\n",
    "gamma = torch.ones(1, 5)\n",
    "beta = torch.zeros(1, 5)\n",
    "\n",
    "y = gamma * x_norm + beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "3efc0a0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.8215e+00, 2.4494e-04, 6.4822e+00, 8.9441e-01])\n",
      "tensor([ 1.1286, -0.0090,  1.4699,  0.5460], requires_grad=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9t/78vgfzf94l3f7wnvfk1ng3lc0000gq/T/ipykernel_66975/1635869463.py:16: DeprecationWarning: <class '__main__.MyGradient'> should not be instantiated. Methods on autograd functions are all static, so you should invoke them on the class itself. Instantiating an autograd function will raise an error in a future version of PyTorch.\n",
      "  f = MyGradient()\n"
     ]
    }
   ],
   "source": [
    "class MyGradient(torch.autograd.Function):\n",
    "    # this does not need a self arggument\n",
    "    @staticmethod \n",
    "    def forward (ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        # fx = x 3\n",
    "        return input**3 \n",
    "    @staticmethod \n",
    "    def backward(ctx, grad_output):\n",
    "        x, = ctx.saved_tensors()\n",
    "        return grad_output*3*x*x+0.5 \n",
    "x = torch.randn(4, requires_grad=True)\n",
    "y = torch.sum(x**3)\n",
    "y.backward()\n",
    "print(x.grad)\n",
    "f = MyGradient()\n",
    "g = f.apply(x)\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "9bbddecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚öôÔ∏è Level 7 ‚Äî ‚ÄúAdvanced Pain but Beautiful‚Äù\n",
    "# 1. Custom Autograd Function: SoftClipped ReLU\n",
    "\n",
    "# x,0.01‚ãÖtanh(x),x>0 x‚â§0\t‚Äã\n",
    "\n",
    "class CustomAutograd(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        ctx.save_for_backward(input)\n",
    "        y = torch.where(input>0, input, 0.01*torch.tanh(input))\n",
    "        return y \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        (x,) = ctx.saved_tensors\n",
    "        # derivative piecewise: 1 for x>0, 0.01*(1 - tanh(x)^2) otherwise\n",
    "        neg_grad = 0.01 * (1 - torch.tanh(x).pow(2))\n",
    "        df_dx = torch.where(x > 0, torch.ones_like(x), neg_grad)\n",
    "        grad_input = grad_output * df_dx\n",
    "        return grad_input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "a925b6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 1])\n",
      "torch.Size([5, 1])\n",
      "tensor(4.5710, grad_fn=<MseLossBackward0>)\n",
      "torch.Size([5, 1])\n",
      "tensor(4.0679, grad_fn=<MseLossBackward0>)\n",
      "torch.Size([5, 1])\n",
      "tensor(3.6332, grad_fn=<MseLossBackward0>)\n",
      "torch.Size([5, 1])\n",
      "tensor(3.2568, grad_fn=<MseLossBackward0>)\n",
      "torch.Size([5, 1])\n",
      "tensor(2.9304, grad_fn=<MseLossBackward0>)\n",
      "torch.Size([5, 1])\n",
      "tensor(2.6466, grad_fn=<MseLossBackward0>)\n",
      "torch.Size([5, 1])\n",
      "tensor(2.3993, grad_fn=<MseLossBackward0>)\n",
      "torch.Size([5, 1])\n",
      "tensor(2.1833, grad_fn=<MseLossBackward0>)\n",
      "torch.Size([5, 1])\n",
      "tensor(1.9941, grad_fn=<MseLossBackward0>)\n",
      "torch.Size([5, 1])\n",
      "tensor(1.8279, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# linear regression training loop \n",
    "x = torch.randn(5, 10)\n",
    "# bacthsize, features \n",
    "y = (3 * x + 2 + torch.randn_like(x)*0.1).mean(dim=1, keepdim=True)  # (5,1)\n",
    "\n",
    "print(y.shape)\n",
    "model = torch.nn.Linear(10,1)\n",
    "loss = torch.nn.MSELoss()\n",
    "optim = torch.optim.SGD(model.parameters(),lr = 0.01)\n",
    "for _ in range(10):\n",
    "    y_hat = model(x)\n",
    "    print(y_hat.shape) # 5,1 \n",
    "    \n",
    "    loss_ = loss(y,y_hat )\n",
    "    print(loss_)\n",
    "    optim.zero_grad()\n",
    "    loss_.backward()\n",
    "    optim.step()\n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a5d54ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
