{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4c5b1fc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    \"\"\"\n",
    "   Calculates the scaled dot-product attention.\n",
    "\n",
    "   Args:\n",
    "    query: Query tensor; shape (batch_size, num_heads, seq_len_q, d_k)\n",
    "    key: Key tensor; shape (batch_size, num_heads, seq_len_k, d_k)\n",
    "    value: Value tensor; shape (batch_size, num_heads, seq_len_v, d_v)\n",
    "           Note: seq_len_k and seq_len_v must be the same.\n",
    "    mask: Optional mask tensor; shape can be broadcastable to\n",
    "          (batch_size, num_heads, seq_len_q, seq_len_k).\n",
    "\n",
    "   Returns:\n",
    "    A tuple containing:\n",
    "    - output: The attention-weighted value tensor;\n",
    "              shape (batch_size, num_heads, seq_len_q, d_v)\n",
    "    - attention_weights: The attention weights;\n",
    "                         shape (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    \"\"\"\n",
    "    dk = key.shape[-1]\n",
    "    scores = torch.matmul(query, key.transpose(-1, -2)) # output shape will be  (batch_size, num_heads, seq_len_q, seq_len_k)\n",
    "    scores = scores/ math.sqrt(dk)\n",
    "\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask, -1e9) # maksed fill more common than assing inf \n",
    "    assert value.shape[-2] == scores.shape[-1]\n",
    "    scores = torch.softmax(scores, dim=-1)   # Softmax is applied on the last dimension (seq_len_k) to get weights for each query\n",
    "\n",
    "    attention_weights = torch.matmul(scores, value) # shape (batch_size, num_heads, seq_len_q, d_v)\n",
    "    return attention_weights, scores\n",
    "    \n",
    "#Complexity: What is the time and space complexity of this function with respect to the sequence length, N (assuming seq_len_q = seq_len_k = N)?\n",
    "#The Scaling Factor: Why do we divide by dk? What would happen if we didn't?\n",
    "\n",
    "#Masking: In the context of a vanilla Transformer decoder (like in GPT), what is the specific name for the mask you would use here, and what is its purpose?\n",
    "# time complexity O(N2) matmul are O(n2 )\n",
    "# sclae by dk to get stable training , dk is the variance for the batch size \n",
    "# causal MAsk for decoder, and this is doen so that the attention is only applied to the previosu tokens when processing the current tokens\n",
    "# this presebves the causal nature of the  \n",
    "\n",
    "\n",
    "# Detailed Breakdown:\n",
    "# The key is to understand what happens to the softmax function with large inputs.\n",
    "# The Problem: Let's assume the components of the query and key vectors are independent random variables with a mean of 0 and a variance of 1. The dot product of two such vectors, qâ‹…k, will have a mean of 0 but a variance of d \n",
    "# k.The Consequence: If dk is large (e.g., 512), the dot product scores can have a very large variance, meaning some values will be very large and others very small. When you feed these large-magnitude numbers into a softmax function, it pushes the probabilities to either 0 or 1.\n",
    "# Vanishing Gradients: When the softmax output is saturated at 0 or 1, its gradient becomes extremely close to zero. This is the \"vanishing gradient\" problem. It means that very little signal flows back during backpropagation, and the model struggles to learn.\n",
    "# The Solution: By dividing the scores by  d k(the standard deviation), we scale the variance back down to 1, keeping the inputs to the softmax in a more reasonable range. This leads to healthier gradients and more stable training.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d5237d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFollow ups:\\nQuestion: Why do we use one big nn.Linear(d_model, d_model) and then reshape, instead of creating num_heads smaller nn.Linear(d_k, d_k) layers and running them in a loop?\\none large matrix multiplication is faster and efficient than num_heads small matrix multiplications\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multihead attention\n",
    "#Instead of performing a single attention calculation, Multi-Head Attention runs the scaled dot-product attention\n",
    "# mechanism multiple times in parallel. Each parallel run is called a \"head.\" This allows the model to jointly attend \n",
    "# to information from different representation subspaces at different positions. It's like having a committee of \n",
    "# experts; each expert (head) focuses on a different aspect of the input, and their insights are combined for a final\n",
    "# decision.\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: The dimensionality of the input and output.\n",
    "            num_heads: The number of attention heads.\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "\n",
    "        self.w_q = nn.Linear(self.d_model, self.d_model)\n",
    "        self.w_k = nn.Linear(self.d_model, self.d_model)\n",
    "        self.w_v = nn.Linear(self.d_model, self.d_model)\n",
    "        self.w_o = nn.Linear(self.d_model, self.d_model) #this takes the final layers as input after combining the heads and then reweightign the alyers \n",
    "\n",
    "    def forward(self, query, key, value, mask ):\n",
    "\n",
    "        # shape of these is (batch_size, seq_len_q, d_model)\n",
    "        # split them into heads and dk \n",
    "        Q = self.w_q(query)\n",
    "        K = self.w_k(key)\n",
    "        V = self.w_v(value)\n",
    "\n",
    "        batch_size = query.shape(0)\n",
    "        Q = Q.view(batch_size, self.num_heads,  -1, self.d_k).transpose(1,2)\n",
    "        V = V.view(batch_size, self.num_heads,  -1, self.d_k).transpose(1,2)\n",
    "        K = K.view(batch_size, self.num_heads,  -1, self.d_k).transpose(1,2)\n",
    "\n",
    "\n",
    "        # now we have input for each head the attentino works on these now \n",
    "        output, scores = scaled_dot_product_attention(Q, K, V, mask=None)\n",
    "        # output shape is same as query (batch_size, self.num_heads, seq_len  , self.d_k)\n",
    "        output = output.transpose(1,2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        output = self.w_o(output)\n",
    "        return output, scores\n",
    "'''\n",
    "torch.view() requires the tensor to be contiguous in memory.\n",
    " A contiguous tensor is one where all its elements are stored sequentially in a single block of memory.\n",
    "   It doesn't create a new copy of the data; it just changes how PyTorch \"views\" the existing memory block, \n",
    "   making it very fast and memory-efficient.\n",
    "\n",
    "torch.reshape() is more flexible. If the tensor is already contiguous, it acts just like view(). \n",
    "However, if the tensor is not contiguous (e.g., after a transpose() operation), reshape() will implicitly\n",
    " create a copy of the tensor with a contiguous layout before reshaping it.\n",
    "'''\n",
    "'''\n",
    "Follow ups:\n",
    "Question: Why do we use one big nn.Linear(d_model, d_model) and then reshape, instead of creating num_heads smaller nn.Linear(d_k, d_k) layers and running them in a loop?\n",
    "one large matrix multiplication is faster and efficient than num_heads small matrix multiplications\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "01e3e4f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# building the whole transformer block \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# Assume the MultiHeadAttention class we built is available\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: The dimensionality of the input and output (must be the same).\n",
    "            num_heads: The number of attention heads.\n",
    "            d_ff: The dimensionality of the inner layer of the FFN.\n",
    "            dropout: The dropout rate.\n",
    "        \"\"\"\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.norm1 = torch.nn.LayerNorm(d_model)\n",
    "        self.norm2 = torch.nn.LayerNorm(d_model)\n",
    "        self.ffn  = nn.Sequential(torch.nn.Linear(d_model, 4*d_model),\n",
    "                                  torch.nn.ReLU(),\n",
    "                                  torch.nn.Linear(4*d_model, d_model))\n",
    "        self.dropout = nn.Dropout(dropout=dropout)\n",
    "    def forward(self, x, mask = None):\n",
    "        # x is the embedding \n",
    "        # norm -> sublayer -> dropout -> add residual\n",
    "        residual = x\n",
    "        # (batch_size, seq_len, d_model)\n",
    "        x_norm = self.norm1(x)\n",
    "        attention_output, weights = self.attention(x_norm,x_norm, x_norm, mask)\n",
    "        x = residual  + self.dropout(attention_output)\n",
    "        # pre norm \n",
    "        residual = x \n",
    "        x_norm = self.norm2(x)\n",
    "        # combine first and then do norm\n",
    "        ffn_output = self.ffn(x_norm)\n",
    "        x = residual+ self.dropout(ffn_output )\n",
    "        return x\n",
    "\n",
    "\n",
    "# whihc is stable pre LN or Post LN\n",
    "# The key problem Pre-LN solves is the exploding gradient issue in very deep Transformers.\n",
    "# In the Post-LN architecture, the output of each block is normalized, but the residual connection path is not. As you stack many blocks, the gradients flowing backward through the additions (+) can accumulate and become very large, leading to unstable training.\n",
    "\n",
    "#In the Pre-LN architecture, the normalization layer is placed directly on the main residual path. \n",
    "# This means that at the start of every block, the gradients are \"reset\" or rescaled by the LayerNorm.\n",
    "#  This keeps the gradient magnitudes well-behaved throughout the entire network, allowing for much more stable training, \n",
    "# especially for models with dozens or hundreds of layers. It often removes the need for learning rate warm-up schedules. ðŸ“ˆ\n",
    "\n",
    "\n",
    "# what is hte role of FFN?\n",
    "# it adds non linearlity to the otherwise linear attentoin block , \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a3b178f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# positional encoding \n",
    "# shape max_seq_len x d_model \n",
    "# uses self.register_buffer('pe', pe) to tell torch to not include this in the trainable parameters, these are fixed paramters defined for each index in the seq length \n",
    "# lets make the original positional encoding matrix now \n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_len= 100000):\n",
    "        super().__init__()\n",
    "        pe = torch.randn(d_model, max_seq_len)\n",
    "        position  = torch.arange(0, max_seq_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "    def forward(self, x):\n",
    "        # we simply add the pe to x \n",
    "        # shape of x batch_size, seq, d \n",
    "        # shape of pe 1, seq, max_seq_len\n",
    "        self.pe = self.pe.unsqueeze(0)\n",
    "        x = x + self.pe[:, :x.size(1)]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0ccd23b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Assume TransformerEncoderBlock and PositionalEncoding are defined from previous exercises\n",
    "\n",
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, dropout=0.1, max_seq_len=5000):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: The size of the vocabulary.\n",
    "            d_model: The dimensionality of the embeddings.\n",
    "            num_heads: The number of attention heads.\n",
    "            d_ff: The dimensionality of the inner layer of the FFN.\n",
    "            num_layers: The number of TransformerEncoderBlocks to stack.\n",
    "            dropout: The dropout rate.\n",
    "            max_seq_len: The maximum possible sequence length.\n",
    "        \"\"\"\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "\n",
    "        # Your code here:\n",
    "        # 1. An embedding layer for the input tokens.\n",
    "        # self.embedding = ...\n",
    "\n",
    "        # 2. A positional encoding layer.\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_seq_len)\n",
    "\n",
    "        # 3. A stack of N encoder blocks. Use nn.ModuleList.\n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "            [TransformerEncoderBlock(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "        # 4. A dropout layer for the embeddings.\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "\n",
    "    def forward(self, src, src_mask=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            src: The input token IDs; shape (batch_size, seq_len)\n",
    "            src_mask: The attention mask for the source sequence.\n",
    "\n",
    "        Returns:\n",
    "            The output of the final encoder block; shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Your code here:\n",
    "        # 1. Pass the input through the embedding layer.\n",
    "        # 2. Scale the embeddings by sqrt(d_model) - a common practice.\n",
    "        # 3. Add the positional encodings.\n",
    "        # 4. Apply dropout.\n",
    "        # 5. Pass the result through the stack of encoder layers.\n",
    "        src_emb = self.embedding(src)\n",
    "        src_emb = src_emb * torch.sqrt(self.d_model)\n",
    "        src_emb = self.pos_encoder(src_emb)\n",
    "        output = self.dropout(src_emb)\n",
    "        for layer in self.encoder_layers:\n",
    "            output = layer(output, mask=src_mask)\n",
    "        return output \n",
    "# Embedding scaling \n",
    "# The scaling is done to adjust the relative importance of the token embedding compared to the positional encoding.\n",
    "#Token Embedding: The nn.Embedding layer is typically initialized with weights from a standard normal distribution (mean 0, variance 1).\n",
    "# Positional Encoding: The values from our sine/cosine functions are always between -1 and 1, and their variance is around 0.5.\n",
    "# Without scaling, when we add these two together, the token embedding has a larger magnitude and might overshadow the positional information.\n",
    "# By scaling the embedding by  d model, we increase its magnitude. This makes the positional encoding a smaller, more subtle signal that is added to the much stronger token signal.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f72b3d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# decoder only block \n",
    "class DecoderOnlyBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, num_heads, dropout):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model \n",
    "        self.d_k  = d_model/ num_heads \n",
    "        self.num_heads = num_heads\n",
    "        # self.w_q = nn.Linear(d_model, d_model )\n",
    "        # self.w_k = nn.Linear(d_model, d_model )\n",
    "        # self.w_v = nn.Linear(d_model, d_model)\n",
    "        # self.W_o = nn.Linear(d_model, d_model)\n",
    "        self.attention = MultiHeadAttention(d_model, num_heads)\n",
    "        self.norm1 = torch.nn.LayerNorm(d_model)\n",
    "        self.norm2 = torch.nn.LayerNorm(d_model)\n",
    "        self.ffn = nn.Sequential(nn.Linear(d_model, 4*d_model), nn.ReLU(), nn.Linear(d_model*4, d_model))\n",
    "        self.dropout = nn.Dropout(dropout =dropout )\n",
    "    def forward(self, x,  mask):\n",
    "        residual = x\n",
    "        x_norm = self.norm1(x)\n",
    "        x_attention, _ = self.attention(x_norm, x_norm, x_norm,mask )\n",
    "        x = residual + self.dropout(x_attention)\n",
    "\n",
    "        residual = x \n",
    "        x_norm = self.norm2(x)\n",
    "        ffn_output = self.ffn(x_norm)\n",
    "        x = residual + self.dropout(ffn_output)\n",
    "        return x \n",
    "\n",
    "        # q = self.w_q(query)\n",
    "        # k = self.w_k(key)\n",
    "        # v = self.w_v(value)\n",
    "        # batch_size = q.shape(0)\n",
    "        # seq_len = q.shape(1)\n",
    "\n",
    "        # # now we can split over batch size, seq_len, d_model to batch size, num_heads, seq_len, d_k\n",
    "        # q = q.view(batch_size, -1, self.num_heads, self.d_k ).transpose(1, 2)\n",
    "        # k = k.view(batch_size, -1, self.num_heads, self.d_k ).transpose(1, 2)\n",
    "        # v = v.view(batch_size, -1, self.num_heads, self.d_k ).transpose(1, 2)\n",
    "\n",
    "        # # now pass them through the attention block \n",
    "        # # we need to define the mask as well \n",
    "        # mask = torch.ones(seq_len, seq_len )\n",
    "        # # keep only the lower traingular matrix \n",
    "        # # idk hwo to do this \n",
    "        # attention_output,_ = scaled_dot_product_attention(q,k,v, mask = mask )\n",
    "        # # this will be in the shape of batch size, num_heads, seq_len, d_k\n",
    "        # # now combien them and then pass the Wo matrix \n",
    "\n",
    "        # attention_output = attention_output.transpose(1,2)\n",
    "        # attention_output = attention_output.contiguous().view(batch_size, -1, self.d_model)\n",
    "        # output = self.W_o(attention_output)\n",
    "\n",
    "        return output \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "925ac9d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False,  True,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False,  True,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False,  True,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False,  True,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False,  True,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False,  True,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False,  True,  True],\n",
       "        [False, False, False, False, False, False, False, False, False,  True],\n",
       "        [False, False, False, False, False, False, False, False, False, False]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask = torch.triu(torch.ones(10, 10), diagonal=1)\n",
    "mask = mask.bool()\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "28861ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gpt problem \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# Assume DecoderOnlyBlock and PositionalEncoding are defined from previous exercises\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, num_layers, dropout, max_seq_len):\n",
    "        super().__init__() # to properly initilaise the nn.Module class \n",
    "        self.d_model = d_model \n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_heads = num_heads\n",
    "        self.dk = d_model/num_heads\n",
    "        self.token_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, max_seq_len)\n",
    "        self.decoder_blocks = nn.ModuleList([DecoderOnlyBlock(d_model, num_heads, dropout) for _ in range(num_layers)])\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "         \n",
    "        self.final_norm = nn.LayerNorm(d_model)\n",
    "        self.lm_head = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # (batch_size, seq_len)\n",
    "        x = self.token_embedding(x)* math.sqrt(self.d_model)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.dropout(x)\n",
    "        # pass through attention \n",
    "        for layer in self.decoder_blocks: \n",
    "            x = layer(x,mask = mask )\n",
    "        # final layernorm \n",
    "        output = self.final_norm(output)\n",
    "        output = self.lm_head(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "53ec8fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM inference and optimisations\n",
    "# KV cache , optimises the token generation pipeline ofro N tokens from O(N3) summation O(n2) for each token = n3 to \n",
    "# O(n) for each n tokens = O(n2) complexity \n",
    "class MultiHeadAttentionWithKVCache(nn.Module):\n",
    "    def __init__(self,  d_model, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        # define your QKV queries \n",
    "        self.w_q = nn.Linear(d_model, d_model )\n",
    "        self.w_k = nn.Linear(d_model, d_model )\n",
    "        self.w_v = nn.Linear(d_model, d_model )\n",
    "        self.w_o = nn.Linear(d_model, d_model )\n",
    "        self.d_model = d_model \n",
    "        self.num_heads = num_heads \n",
    "        self.dk = self.d_model//num_heads\n",
    "\n",
    "    def forward(self,query, key, value, mask=None, kv_cache=None):\n",
    "        batch_size= key.size(0)\n",
    "        Q = self.w_q(query).view(batch_size, -1, self.num_heads,self.dk ).transpose(1,2)\n",
    "        key_new = self.w_k(key).view(batch_size, -1, self.num_heads,self.dk ).transpose(1,2)\n",
    "        value_new = self.w_v(value).view(batch_size, -1, self.num_heads,self.dk ).transpose(1,2)\n",
    "        print(\"key_new\", key_new.shape)\n",
    "        if kv_cache is not None:\n",
    "            # concatenate the new KV values \n",
    "            key_cache, value_cache = kv_cache\n",
    "            print(key_cache.shape)\n",
    "            K = torch.cat([key_cache, key_new],dim=2)\n",
    "            V = torch.cat([value_cache, value_new],dim=2)\n",
    "            print(K.shape)\n",
    "            print(V.shape)\n",
    "        else:\n",
    "            K,V = key_new, value_new\n",
    "        updated_kv_cache = (K, V)\n",
    "        context, _ = scaled_dot_product_attention(Q, K, V, mask)\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        output = self.w_o(context)\n",
    "        return output, updated_kv_cache\n",
    "\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "191efc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([1, 5, 12])\n",
      "key_new torch.Size([1, 4, 5, 3])\n",
      "torch.Size([1, 5, 12])\n",
      "torch.Size([1, 4, 5, 3])\n",
      "torch.Size([1, 4, 5, 3])\n",
      "--- Step 2: Generating the 1st New Token ---\n",
      "Input shape: torch.Size([1, 1, 12])\n",
      "key_new torch.Size([1, 4, 1, 3])\n",
      "torch.Size([1, 4, 5, 3])\n",
      "torch.Size([1, 4, 6, 3])\n",
      "torch.Size([1, 4, 6, 3])\n",
      "torch.Size([1, 4, 6, 3])\n",
      "Input shape: torch.Size([1, 1, 12])\n",
      "key_new torch.Size([1, 4, 1, 3])\n",
      "torch.Size([1, 4, 6, 3])\n",
      "torch.Size([1, 4, 7, 3])\n",
      "torch.Size([1, 4, 7, 3])\n",
      "Output shape: torch.Size([1, 1, 12])\n",
      "Returned Key Cache shape: torch.Size([1, 4, 7, 3])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "d_model = 12\n",
    "num_heads = 4\n",
    "seq_len_prompt = 5\n",
    "mha_kv = MultiHeadAttentionWithKVCache(d_model=d_model, num_heads=num_heads)\n",
    "prompt = torch.randn(batch_size, seq_len_prompt, d_model)\n",
    "print(f\"Input shape: {prompt.shape}\")\n",
    "# First forward pass with no cache\n",
    "output, kv_cache = mha_kv(prompt, prompt, prompt, kv_cache=None)\n",
    "print(output.shape)\n",
    "print(kv_cache[0].shape)\n",
    "print(kv_cache[1].shape)\n",
    "print(\"--- Step 2: Generating the 1st New Token ---\")\n",
    "# Create a tensor for a single new token\n",
    "new_token_1 = torch.randn(batch_size, 1, d_model)\n",
    "print(f\"Input shape: {new_token_1.shape}\")\n",
    "output, kv_cache = mha_kv(new_token_1, new_token_1, new_token_1, kv_cache=kv_cache)\n",
    "print(kv_cache[0].shape)\n",
    "\n",
    "new_token_2 = torch.randn(batch_size, 1, d_model)\n",
    "print(f\"Input shape: {new_token_2.shape}\")\n",
    "# Third forward pass, feeding the updated cache\n",
    "output, kv_cache = mha_kv(new_token_2, new_token_2, new_token_2, kv_cache=kv_cache)\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Returned Key Cache shape: {kv_cache[0].shape}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541d3f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature and Top-K Sampling\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def sample_top_k_with_temperature(logits, temperature=1.0, k=50):\n",
    "    # logits shape 1 x V \n",
    "    if temperature ==0 :\n",
    "        sample = torch.argmax(logits,dim=-1)\n",
    "        # decode this \n",
    "        return sample.item()\n",
    "    logits = logits / temperature\n",
    "    top_k_logits ,top_k_indices = torch.topk(logits)\n",
    "    \n",
    "    top_k_probs =  F.softmax(top_k_logits, dim=-1)\n",
    "    sampled_index_in_top_k = torch.multinomial(top_k_probs, num_samples=1)\n",
    "    final_token_id = top_k_indices[0, sampled_index_in_top_k.item()].item()\n",
    "    return final_token_id\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cb7a5f59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.sort(\n",
       "values=tensor([[-1.4046, -1.1718, -1.1229, -0.8169, -0.7663, -0.7543, -0.6873, -0.4600,\n",
       "          1.1832,  1.4680]]),\n",
       "indices=tensor([[3, 8, 9, 2, 5, 6, 0, 7, 4, 1]]))"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits = torch.randn(1, 10)\n",
    "logits = torch.sort(logits)\n",
    "logits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523269f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 9, 2, 7, 3, 5, 1, 8, 4, 6]])\n",
      "tensor([[ 0.9664,  0.8910,  0.4614,  0.2874,  0.1262, -0.1792, -0.3994, -0.8468,\n",
      "         -1.2610, -1.5057]])\n",
      "tensor([[0.2274, 0.2109, 0.1372, 0.1153, 0.0981, 0.0723, 0.0580, 0.0371, 0.0245,\n",
      "         0.0192]])\n"
     ]
    }
   ],
   "source": [
    "# Temperature and nucleus Sampling where k is not fixed so allows for more creativity \n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "logits = torch.randn(1, 10)\n",
    "\n",
    "# def sample_top_p(logits, p=0.9):    \n",
    "# sort the logits \n",
    "sorted_logits, sorted_indices = torch.sort(logits, descending=True, dim=-1)\n",
    "print(sorted_indices)\n",
    "print(sorted_logits)\n",
    "# sorted probs wiht htis \n",
    "sorted_probs = torch.softmax(sorted_logits, dim=-1)\n",
    "print(sorted_probs)\n",
    "cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "# 4. Create a boolean mask for tokens to remove.\n",
    "# We want to remove all tokens that appear *after* the cumulative\n",
    "# probability has exceeded the threshold 'p'.\n",
    "# We shift the cumulative probabilities right by one and compare to 'p'.\n",
    "# This ensures that the token that pushes the sum *over* p is included.\n",
    "shifted_cumulative_probs = F.pad(cumulative_probs[:, :-1], (1, 0), \"constant\", 0)\n",
    "indices_to_remove = shifted_cumulative_probs > p\n",
    "\n",
    "# 5. Set the logits of the tokens to be removed to negative infinity.\n",
    "# This effectively removes them from the running for the next softmax.\n",
    "# We use the sorted_indices to find the correct original positions to mask.\n",
    "# The .scatter_ method is a clean way to apply this mask.\n",
    "logits[indices_to_remove.scatter(1, sorted_indices, indices_to_remove)] = -float('Inf')\n",
    "final_probs = F.softmax(logits, dim=-1)\n",
    "    \n",
    "# 7. Sample one token from this final, filtered distribution.\n",
    "sampled_token_id = torch.multinomial(final_probs, num_samples=1)\n",
    "\n",
    "print(sampled_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0713c82a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
