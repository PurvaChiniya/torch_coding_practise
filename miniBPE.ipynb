{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2f18866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenization \n",
    "# text from https://www.reedbeta.com/blog/programmers-intro-to-unicode/\n",
    "text = \"ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.\"\n",
    "tokens = text.encode(\"utf-8\") # raw bytes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "406e5d6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.\n",
      "length: 533\n",
      "533\n",
      "--machine readable tokens is utf encoding so the no of bytes read by the machines is more\n",
      "[239, 188, 181, 239, 189, 142, 239, 189, 137, 239, 189, 131, 239, 189, 143, 239, 189, 132, 239, 189, 133, 33, 32, 240, 159, 133, 164, 240, 159, 133, 157, 240, 159, 133, 152, 240, 159, 133, 146, 240, 159, 133, 158, 240, 159, 133, 147, 240, 159, 133, 148, 226, 128, 189, 32, 240, 159, 135, 186, 226, 128, 140, 240, 159, 135, 179, 226, 128, 140, 240, 159, 135, 174, 226, 128, 140, 240, 159, 135, 168, 226, 128, 140, 240, 159, 135, 180, 226, 128, 140, 240, 159, 135, 169, 226, 128, 140, 240, 159, 135, 170, 33, 32, 240, 159, 152, 132, 32, 84, 104, 101, 32, 118, 101, 114, 121, 32, 110, 97, 109, 101, 32, 115, 116, 114, 105, 107, 101, 115, 32, 102, 101, 97, 114, 32, 97, 110, 100, 32, 97, 119, 101, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 104, 101, 97, 114, 116, 115, 32, 111, 102, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 119, 111, 114, 108, 100, 119, 105, 100, 101, 46, 32, 87, 101, 32, 97, 108, 108, 32, 107, 110, 111, 119, 32, 119, 101, 32, 111, 117, 103, 104, 116, 32, 116, 111, 32, 226, 128, 156, 115, 117, 112, 112, 111, 114, 116, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 157, 32, 105, 110, 32, 111, 117, 114, 32, 115, 111, 102, 116, 119, 97, 114, 101, 32, 40, 119, 104, 97, 116, 101, 118, 101, 114, 32, 116, 104, 97, 116, 32, 109, 101, 97, 110, 115, 226, 128, 148, 108, 105, 107, 101, 32, 117, 115, 105, 110, 103, 32, 119, 99, 104, 97, 114, 95, 116, 32, 102, 111, 114, 32, 97, 108, 108, 32, 116, 104, 101, 32, 115, 116, 114, 105, 110, 103, 115, 44, 32, 114, 105, 103, 104, 116, 63, 41, 46, 32, 66, 117, 116, 32, 85, 110, 105, 99, 111, 100, 101, 32, 99, 97, 110, 32, 98, 101, 32, 97, 98, 115, 116, 114, 117, 115, 101, 44, 32, 97, 110, 100, 32, 100, 105, 118, 105, 110, 103, 32, 105, 110, 116, 111, 32, 116, 104, 101, 32, 116, 104, 111, 117, 115, 97, 110, 100, 45, 112, 97, 103, 101, 32, 85, 110, 105, 99, 111, 100, 101, 32, 83, 116, 97, 110, 100, 97, 114, 100, 32, 112, 108, 117, 115, 32, 105, 116, 115, 32, 100, 111, 122, 101, 110, 115, 32, 111, 102, 32, 115, 117, 112, 112, 108, 101, 109, 101, 110, 116, 97, 114, 121, 32, 97, 110, 110, 101, 120, 101, 115, 44, 32, 114, 101, 112, 111, 114, 116, 115, 44, 32, 97, 110, 100, 32, 110, 111, 116, 101, 115, 32, 99, 97, 110, 32, 98, 101, 32, 109, 111, 114, 101, 32, 116, 104, 97, 110, 32, 97, 32, 108, 105, 116, 116, 108, 101, 32, 105, 110, 116, 105, 109, 105, 100, 97, 116, 105, 110, 103, 46, 32, 73, 32, 100, 111, 110, 226, 128, 153, 116, 32, 98, 108, 97, 109, 101, 32, 112, 114, 111, 103, 114, 97, 109, 109, 101, 114, 115, 32, 102, 111, 114, 32, 115, 116, 105, 108, 108, 32, 102, 105, 110, 100, 105, 110, 103, 32, 116, 104, 101, 32, 119, 104, 111, 108, 101, 32, 116, 104, 105, 110, 103, 32, 109, 121, 115, 116, 101, 114, 105, 111, 117, 115, 44, 32, 101, 118, 101, 110, 32, 51, 48, 32, 121, 101, 97, 114, 115, 32, 97, 102, 116, 101, 114, 32, 85, 110, 105, 99, 111, 100, 101, 226, 128, 153, 115, 32, 105, 110, 99, 101, 112, 116, 105, 111, 110, 46]\n",
      "length: 616\n",
      "ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.\n",
      "533\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "tokens = list(map(int, tokens)) # convert to a list of integers in range 0..255 for convenience\n",
    "print('---')\n",
    "print(text)\n",
    "print(\"length:\", len(text))\n",
    "print(len(text.strip()))\n",
    "\n",
    "print('--machine readable tokens is utf encoding so the no of bytes read by the machines is more')\n",
    "print(tokens)\n",
    "print(\"length:\", len(tokens))\n",
    "\n",
    "# we can decode back to text using the tokens \n",
    "decoded = bytes(tokens).decode(\"utf-8\")\n",
    "print(decoded)\n",
    "print(len(decoded))\n",
    "print(decoded == text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "92a6a83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BPE algorithm \n",
    "# it finds the most frequent pair from get stats \n",
    "# assign it a new token id \n",
    "# merge it into vocab \n",
    "# repeat this N times \n",
    "\n",
    "# lets write the steps \n",
    "# find the most freq pair \n",
    "def stats(tokens):\n",
    "    # given tokens as a list of all the possible tokens find out the most freq token or get the stats \n",
    "    counts = {}\n",
    "    for pair in zip(tokens, tokens[1:]):\n",
    "        counts[pair] = counts.get(pair, 0 )+1 \n",
    "    return counts \n",
    "# # get the top pair \n",
    "# stats_tokens = stats(tokens)\n",
    "# top_pair = max(stats_tokens, key = stats_tokens.get)\n",
    "# # now merge this toppair to the vocab, do we remove the individual pair elements --no ??\n",
    "def merge(tokens, new_pair, idx  ):\n",
    "    newids = []\n",
    "    # this idx will be the max(tokens)+1 \n",
    "    # idx = max(tokens)+1\n",
    "    # # each new merge will have a new idx \n",
    "    i = 0 \n",
    "    while i < len(tokens):\n",
    "        if i < len(tokens) - 1  and  tokens[i]==new_pair[0] and tokens[i+1]==new_pair[1]:\n",
    "            # we skip the occurence of the new_pair and instead just keep the occurence of this pair in the string here \n",
    "            newids.append(idx)\n",
    "            i+=2 \n",
    "        else:\n",
    "            newids.append(tokens[i])\n",
    "            i+=1\n",
    "    return newids\n",
    "def train_bpe(iterations, text):\n",
    "\n",
    "    merges = []    \n",
    "    id_to_pair = {}\n",
    "    # lets say we have the tokens as the initial tokens \n",
    "    tokens = list(text.encode(\"utf-8\"))\n",
    "    # we want to build the BPE for this by iterating for 10 steps \n",
    "    for _ in range(iterations):\n",
    "        # print(\"we start with total vocab \", len(tokens))\n",
    "        freq_tokens = stats(tokens)\n",
    "        # get the max token here \n",
    "        top_pair = max(freq_tokens, key = freq_tokens.get)\n",
    "        print(id_to_pair)\n",
    "        new_id = max(tokens)+1\n",
    "        merges.append((top_pair, new_id))\n",
    "        id_to_pair[new_id]= top_pair\n",
    "        # now we get the max id and the numebr of times this merge hapeps \n",
    "        # print(\"ids replaced:\", top_pair)\n",
    "        # print(\"coutn by which our vocab decreases\", freq_tokens[top_pair])\n",
    "        # print(\"The â€œ+1â€ you were trying to add belongs to vocabulary size, not sequence length\")\n",
    "        tokens = merge(tokens, top_pair, new_id )\n",
    "        # tokens here is the training text encoded under the learned merges\n",
    "    return tokens, merges, id_to_pair\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d4d28db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{241: (101, 32)}\n",
      "{241: (101, 32), 242: (240, 159)}\n",
      "{241: (101, 32), 242: (240, 159), 243: (226, 128)}\n",
      "{241: (101, 32), 242: (240, 159), 243: (226, 128), 244: (105, 110)}\n",
      "{241: (101, 32), 242: (240, 159), 243: (226, 128), 244: (105, 110), 245: (115, 32)}\n",
      "{241: (101, 32), 242: (240, 159), 243: (226, 128), 244: (105, 110), 245: (115, 32), 246: (97, 110)}\n",
      "{241: (101, 32), 242: (240, 159), 243: (226, 128), 244: (105, 110), 245: (115, 32), 246: (97, 110), 247: (116, 104)}\n",
      "{241: (101, 32), 242: (240, 159), 243: (226, 128), 244: (105, 110), 245: (115, 32), 246: (97, 110), 247: (116, 104), 248: (242, 133)}\n",
      "{241: (101, 32), 242: (240, 159), 243: (226, 128), 244: (105, 110), 245: (115, 32), 246: (97, 110), 247: (116, 104), 248: (242, 133), 249: (242, 135)}\n",
      "original text encoding: 616\n",
      "508\n"
     ]
    }
   ],
   "source": [
    "final_vocab, merges, id_to_pair =   train_bpe(10, text)\n",
    "# lets get to decoding now \n",
    "# now we have a new tokeniser , we can go ahead and encode and decode with this BPE tokenizer now \n",
    "# merge(tokens, top_pair, new_id )\n",
    "# freq tokens is the list for pair and new_id added to this vocab \n",
    "def encode_bpe(merges , text):\n",
    "    tokens = list(text.encode(\"utf-8\"))\n",
    "    for pair, id in merges: # we track all the merges in the training order \n",
    "        tokens = merge(tokens, pair, id )\n",
    "    return tokens\n",
    "text = \"ï¼µï½ï½‰ï½ƒï½ï½„ï½…! ğŸ…¤ğŸ…ğŸ…˜ğŸ…’ğŸ…ğŸ…“ğŸ…”â€½ ğŸ‡ºâ€ŒğŸ‡³â€ŒğŸ‡®â€ŒğŸ‡¨â€ŒğŸ‡´â€ŒğŸ‡©â€ŒğŸ‡ª! ğŸ˜„ The very name strikes fear and awe into the hearts of programmers worldwide. We all know we ought to â€œsupport Unicodeâ€ in our software (whatever that meansâ€”like using wchar_t for all the strings, right?). But Unicode can be abstruse, and diving into the thousand-page Unicode Standard plus its dozens of supplementary annexes, reports, and notes can be more than a little intimidating. I donâ€™t blame programmers for still finding the whole thing mysterious, even 30 years after Unicodeâ€™s inception.\"\n",
    "encoded_text = encode_bpe(merges, text)\n",
    "print(\"original text encoding:\", len(text.encode(\"utf-8\")))\n",
    "print(len(encoded_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "308da572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "#lets decode now \n",
    "\n",
    "def decode_bpe(tokens, id_to_pair):\n",
    "    # Keep expanding learned tokens until everything is a base byte (<256)\n",
    "    for new_id in sorted(id_to_pair.keys(), reverse=True):\n",
    "        pair = id_to_pair[new_id]\n",
    "        new_tokens = []\n",
    "        i = 0\n",
    "        while i < len(tokens):\n",
    "            if tokens[i] == new_id:\n",
    "                a, b = pair\n",
    "                new_tokens.extend([a, b])\n",
    "                i += 1\n",
    "            else:\n",
    "                new_tokens.append(tokens[i])\n",
    "                i += 1\n",
    "        tokens = new_tokens\n",
    "\n",
    "    # At this point, tokens are all raw byte values\n",
    "    return tokens\n",
    "\n",
    "out = decode_bpe(tokens,id_to_pair )\n",
    "print(len(out)==len(text.encode(\"utf-8\")))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bbb94b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962d0d47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2379c602",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c30dd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29fabd17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
