{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4e061c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ae3bd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bfcdd50f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6., 6., 6.])\n",
      "tensor([1.0000, 0.5000, 0.1000])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(3, requires_grad=True)\n",
    "y = x + 2\n",
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "out.backward(retain_graph=True)     # keep graph alive for another backward\n",
    "print(x.grad)                       # tensor([6., 6., 6.])\n",
    "#x.detach()\n",
    "x.grad = None                       # clear accumulation\n",
    "y.backward(torch.tensor([1., 0.5, 0.1]))\n",
    "print(x.grad)                       # tensor([1.0000, 0.5000, 0.1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da05c2df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([7.0000, 6.5000, 6.1000])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba61a7a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "197.4093475341797\n",
      "tensor(0.9142, requires_grad=True) tensor(0.5646, requires_grad=True) tensor(0.5446, requires_grad=True) tensor(-2.0589, requires_grad=True)\n",
      "563.1483764648438\n",
      "tensor(0.8601, requires_grad=True) tensor(1.3496, requires_grad=True) tensor(0.2722, requires_grad=True) tensor(3.4538, requires_grad=True)\n",
      "1800.62890625\n",
      "tensor(0.8250, requires_grad=True) tensor(-0.0648, requires_grad=True) tensor(0.1096, requires_grad=True) tensor(-6.4812, requires_grad=True)\n",
      "5837.61279296875\n",
      "tensor(0.8013, requires_grad=True) tensor(2.4847, requires_grad=True) tensor(0.0126, requires_grad=True) tensor(11.4234, requires_grad=True)\n",
      "18955.60546875\n",
      "tensor(0.7844, requires_grad=True) tensor(-2.1097, requires_grad=True) tensor(-0.0450, requires_grad=True) tensor(-20.8441, requires_grad=True)\n",
      "61563.44140625\n",
      "tensor(0.7717, requires_grad=True) tensor(6.1705, requires_grad=True) tensor(-0.0791, requires_grad=True) tensor(37.3079, requires_grad=True)\n",
      "199949.015625\n",
      "tensor(0.7615, requires_grad=True) tensor(-8.7516, requires_grad=True) tensor(-0.0991, requires_grad=True) tensor(-67.4928, requires_grad=True)\n",
      "649407.875\n",
      "tensor(0.7527, requires_grad=True) tensor(18.1412, requires_grad=True) tensor(-0.1106, requires_grad=True) tensor(121.3774, requires_grad=True)\n",
      "2109191.5\n",
      "tensor(0.7450, requires_grad=True) tensor(-30.3242, requires_grad=True) tensor(-0.1170, requires_grad=True) tensor(-219.0017, requires_grad=True)\n",
      "6850380.0\n",
      "tensor(0.7378, requires_grad=True) tensor(57.0197, requires_grad=True) tensor(-0.1204, requires_grad=True) tensor(394.4244, requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "#coding practise \n",
    "# 1. manual update gradient descent \n",
    "x = torch.linspace(-3.14, 3.14, 2000)\n",
    "y = torch.sin(x)\n",
    "lr = 0.01\n",
    "a = torch.tensor(1.0, requires_grad=True)\n",
    "b = torch.tensor(1.0, requires_grad=True)\n",
    "c = torch.tensor(1.0, requires_grad=True)\n",
    "d = torch.tensor(1.0, requires_grad=True)\n",
    "epochs = 10 \n",
    "for _ in range(epochs):\n",
    "    y_pred = a + b*x + c*x**2 + d*x**3\n",
    "    loss = (y_pred-y)**2  # shape of both is nx 1 \n",
    "    loss = loss.mean()\n",
    "    print(loss.item())\n",
    "    loss.backward() \n",
    "    with torch.no_grad():\n",
    "        a -= lr*a.grad \n",
    "        b -= lr*b.grad \n",
    "        c -= lr*c.grad \n",
    "        d -= lr*d.grad \n",
    "        a.grad = b.grad = c.grad = d.grad = None\n",
    "    print(a,b,c,d)\n",
    "#Single check for you: why is set_to_none=True often better than grad.zero_() in training loops? \n",
    "# Give me the two practical advantages in one line each.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "a27cb578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Problem 2 ‚Äî Custom autograd op\n",
    "#Implement a custom Function for ùë¶=ùë•3. Use ctx.save_for_backward and write the correct backward.\n",
    "import torch \n",
    "from torch.autograd import Function \n",
    "\n",
    "class CustomAutograd(Function):\n",
    "    def forward(ctx, input ):\n",
    "        # save the input \n",
    "        ctx.save_for_backward(input)\n",
    "        return input**3\n",
    "    def backward(ctx, grad_output):\n",
    "        x, = ctx.saved_tensors\n",
    "        return grad_output* 3*(x**2)\n",
    "    \n",
    "\n",
    "x = torch.tensor([1.0], requires_grad=True)\n",
    "y = CustomAutograd.apply(x)         # <-- use .apply\n",
    "y.backward(torch.ones_like(y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1a731ed5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.]], grad_fn=<MmBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Linear Layer from scratch \n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.empty(out_features, in_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.empty(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        y = input @ self.weight.T\n",
    "        if self.bias: \n",
    "            out = out + self.bias\n",
    "        return y \n",
    "x = torch.Tensor(1, 10 )\n",
    "\n",
    "layer = MyLinear(10, 1)\n",
    "layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f0ec960a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Torch not compiled with CUDA enabled",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m start = time.time()\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m50\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     y = \u001b[43mx_cpu\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m torch.cuda.synchronize()\n\u001b[32m     15\u001b[39m no_pin_time = time.time() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/hoverboard_workspace/mine/.conda/lib/python3.11/site-packages/torch/cuda/__init__.py:403\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    398\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    399\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot re-initialize CUDA in forked subprocess. To use CUDA with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    400\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmultiprocessing, you must use the \u001b[39m\u001b[33m'\u001b[39m\u001b[33mspawn\u001b[39m\u001b[33m'\u001b[39m\u001b[33m start method\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    401\u001b[39m     )\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(torch._C, \u001b[33m\"\u001b[39m\u001b[33m_cuda_getDeviceCount\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m403\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mTorch not compiled with CUDA enabled\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    404\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _cudart \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    405\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\n\u001b[32m    406\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mlibcudart functions unavailable. It looks like you have a broken build?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    407\u001b[39m     )\n",
      "\u001b[31mAssertionError\u001b[39m: Torch not compiled with CUDA enabled"
     ]
    }
   ],
   "source": [
    "#pin memory \n",
    "import torch, time\n",
    "\n",
    "# Make sure CUDA is available\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# A big fake batch\n",
    "x_cpu = torch.randn(1024, 1024, 256)  # ~1GB tensor\n",
    "\n",
    "# Copy without pinned memory\n",
    "start = time.time()\n",
    "for _ in range(50):\n",
    "    y = x_cpu.to(device, non_blocking=False)\n",
    "torch.cuda.synchronize()\n",
    "no_pin_time = time.time() - start\n",
    "\n",
    "# Copy WITH pinned memory\n",
    "x_pinned = x_cpu.pin_memory()\n",
    "start = time.time()\n",
    "for _ in range(50):\n",
    "    y = x_pinned.to(device, non_blocking=True)\n",
    "torch.cuda.synchronize()\n",
    "pin_time = time.time() - start\n",
    "\n",
    "print(f\"Without pin_memory: {no_pin_time:.3f}s\")\n",
    "print(f\"With pin_memory:    {pin_time:.3f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "d949f6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9890]])\n"
     ]
    }
   ],
   "source": [
    "#Problem 4 ‚Äî forward propogation \n",
    "import numpy as np\n",
    "# torch.Tensor is a fatory funciton to make data torch.Tensor(2,3) makes a tensor of shape 2,3 \n",
    "# whereas torch.tensor makes a tensor of [2, 3] converts this to a tensor \n",
    "X = np.array([[2, 3]])\n",
    "W = np.array([[1], [0.5]])\n",
    "b = np.array([1])\n",
    "import torch \n",
    "X = torch.tensor(X, dtype = torch.float32)\n",
    "W = torch.tensor(W, dtype = torch.float32)\n",
    "b = torch.tensor(b, dtype = torch.float32)\n",
    "y = torch.matmul(X, W) + b \n",
    "y = torch.sigmoid(y)\n",
    "print(y)\n",
    "#print(y.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0003cb41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "tensor([4])\n"
     ]
    }
   ],
   "source": [
    "# gini imputiry for a decisin tree \n",
    "import torch\n",
    "labels = torch.tensor([0, 0, 0, 0])\n",
    "n = labels.numel()\n",
    "print(n)\n",
    "max_class = int(labels.max().item()) \n",
    "counts = torch.bincount(labels.to(torch.int64), minlength=max_class + 1)\n",
    "print(counts)\n",
    "probs = counts.float() / n\n",
    "gini = 1.0 - torch.sum(probs * probs).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "0b8b5928",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3775)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_vector = [1.0, 2.0]\n",
    "\n",
    "input_vector = torch.tensor(input_vector, dtype = torch.float32)\n",
    "W = torch.tensor([1,-1], dtype = torch.float32) # 1, 2 \n",
    "b = torch.tensor(0.5,dtype = torch.float32) # 1 \n",
    "relu = torch.nn.Sigmoid()\n",
    "z = torch.matmul(input_vector, W) + b \n",
    "relu(z) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "f75e3e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([1, 2])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([1, 2])\n",
      "tensor([[0.6698, 0.3302]])\n",
      "torch.Size([1, 1])\n"
     ]
    }
   ],
   "source": [
    "# question 5 cmpute the attention scores in torch \n",
    "import torch\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "torch.manual_seed(0)\n",
    "query = torch.tensor([[1.0, 0.0]])\n",
    "key = torch.tensor([[1.0, 0.0], [0.0, 1.0]])\n",
    "value = torch.tensor([[2.0], [3.0]])\n",
    "dk= key.shape[-1]\n",
    "print(dk )\n",
    "print(query.shape)\n",
    "print(key.shape)\n",
    "dot = torch.matmul(query, key.T)\n",
    "print(dot.shape)\n",
    "dot = dot/(dk**0.5) \n",
    "softmax  = torch.nn.Softmax()\n",
    "attention_scores = softmax(dot)\n",
    "print(attention_scores)\n",
    "attention = torch.matmul(attention_scores, value)\n",
    "print(attention.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ddbe03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n",
      "torch.Size([3, 2])\n",
      "Converged after 2 iterations.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "data = np.array([[1, 2], [1, 4], [5, 2]], dtype=float)\n",
    "k = 2\n",
    "X = torch.tensor(data, dtype=torch.float32)   # n d  3 x 2 \n",
    "seed = 4\n",
    "max_iter = 10 \n",
    "#g = torch.Generator().manual_seed(seed)\n",
    "centroids = X[:k].clone() # kx d \n",
    "for iteration in range(max_iter):\n",
    "    # calculate distrance to k points for each of these n points \n",
    "    x2 =  (X**2).sum(dim=1, keepdim=True) \n",
    "    #print(x2.shape) # 3 1 \n",
    "    c2 = (centroids**2).sum(dim=1,keepdim = True) # (1, k)\n",
    "    #print(c2.shape)\n",
    "    product = torch.matmul(X,centroids.T) # nxd dxk\n",
    "\n",
    "    distances = x2 + c2.T - 2*product # nx k \n",
    "    # we have the distances of each x poitn from the c centroids, now we choose \n",
    "    labels = torch.argmin(distances, dim=1)\n",
    "    print(distances.shape) # nx c \n",
    "    # for each n chooose the minimum centroid \n",
    "    new_centroids = centroids.clone()\n",
    "    for i in range(k):\n",
    "        # for each centroid assign n to it if labels[i]==i \n",
    "        points_in_cluster = X[labels == i]\n",
    "        if len(points_in_cluster) > 0:\n",
    "            # Calculate the mean and update the centroid\n",
    "            new_centroids[i] = points_in_cluster.mean(dim=0)\n",
    "    if torch.allclose(centroids, new_centroids):\n",
    "        print(f\"Converged after {iteration + 1} iterations.\")\n",
    "        break\n",
    "    centroids = new_centroids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af66490d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch coding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca7dd37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
