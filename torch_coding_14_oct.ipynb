{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b643b450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ello'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make the full gpt module now \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from attention_blocks import *\n",
    "# add the loss to calculate cross entropy in this case \n",
    "class GPT(nn.Module): \n",
    "    def __init__(self, embed_size, max_seq_len, vocab_size, num_heads, num_layers ):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # nn.Embedding needs a int 64 inputs Because nn.Embedding is a lookup table, not a linear layer.\n",
    "        self.positional_embedding = nn.Embedding(self.max_seq_len, self.embed_size) #lets make this learnable as well\n",
    "        self.attention_blocks = nn.ModuleList([DecoderOnlyBlock(embedding_dim=embed_size, num_heads=num_heads, \n",
    "                                                                dff = 4*embed_size, dropout=0.9) \n",
    "                                                                for _ in range(self.num_layers)])\n",
    "        self.lm_head  = nn.Linear(self.embed_size, self.vocab_size)\n",
    "        self.layernorm = nn.LayerNorm(self.embed_size)\n",
    "        \n",
    "    def forward(self, x, targets=None):\n",
    "        ## FORWARD SHOULD RETURN THE LOSS HERE SO THAT WE CAN DO BACKPROP \n",
    "        # expect x to eb the pretraining data wiht batch size, seq_len \n",
    "        batch_size, seq_len = x.shape\n",
    "        # call the embedding and ocmibine with positional embedding \n",
    "        token_embedding = self.embedding(x) # batch_size, seq_len, embed_size\n",
    "    \n",
    "        pos_indices = torch.arange(seq_len)\n",
    "        positional_embedding = self.positional_embedding(pos_indices)\n",
    "        x = token_embedding+ positional_embedding\n",
    "        \n",
    "        # shape batch_size, seq_len,embed_size\n",
    "        # now we can pass this through the attention modules with the mask \n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(0)\n",
    "         \n",
    "        # 1, 1, max_seq, max_seq\n",
    "        # make the causal maask \n",
    "        for layer in self.attention_blocks:\n",
    "            x = layer(x, mask)\n",
    "            # x same as output batch_size, max_seq, embed_size\n",
    "        output = self.layernorm(x)\n",
    "        # otuput shape batch, seq\n",
    "        logits = self.lm_head(output)\n",
    "\n",
    "        if targets is None: \n",
    "            ce_loss = None\n",
    "        else:\n",
    "            # lets say the targets are next tokens ie the tokens shifted by 1 \n",
    "            # batch, seq_len \n",
    "            # and we get the logits as batch, seq_len, vocab \n",
    "            targets = targets.view(-1)\n",
    "            predictions = logits.view(-1,self.vocab_size) \n",
    "           \n",
    "            loss = nn.CrossEntropyLoss()\n",
    "            ce_loss = loss(predictions, targets)\n",
    "            # this goes in the backward loss.backward()\n",
    "\n",
    "        return logits,ce_loss# here y is the last token prob\n",
    "    # lets add a generate call for this \n",
    "    # this is the decoding step which calls the froward and samples the token from this recursively \n",
    "    # this is the inference code \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature = 1.0):\n",
    "        self.eval()\n",
    "        #idx shape is previous tokens processsed so (batch_size x seq_len )\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.max_seq_len:]\n",
    "            logits ,_ = self.forward(idx_cond) # batch_size, seq_len, vocab_size\n",
    "            \n",
    "            # get logits from the last time step last seq token basically \n",
    "            last_token_logits = logits[:, -1, :] # b , vocab size \n",
    "            \n",
    "            last_token_logits = last_token_logits / temperature\n",
    "            last_token_probs = torch.softmax(last_token_logits, dim= -1) # the softmax is over all the vocab \n",
    "            # sampple the next token torch.multinomial samples an index based on the weights (probabilities)\n",
    "            last_token = torch.multinomial(last_token_probs, num_samples=1) # B x1 \n",
    "            # print(\"logits\", logits.shape)\n",
    "            # print(last_token.shape)\n",
    "            idx = torch.cat((idx, last_token), dim = 1)\n",
    "            # B, S+1\n",
    "        return idx\n",
    "# Character tokenizer \n",
    "import torch \n",
    "# This is not a torch.nn.Module because it doesn't have any learnable parameters.\n",
    "class CharacterTokenizer: \n",
    "    def __init__(self, corpus):\n",
    "        # find all the unique characters in the corpus \n",
    "        # map them to a dictionary \n",
    "        # reverse map it to the tokens \n",
    "        self.vocab = sorted(list(set(corpus.strip())))\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.char_to_index = {char:i for i,char in enumerate(self.vocab)}\n",
    "        self.index_to_char  = {i:char for i,char in enumerate(self.vocab)}\n",
    "    def encode(self, text):\n",
    "        indices = [self.char_to_index[char] for char in text]\n",
    "        return torch.tensor(indices)\n",
    "    def decode(self, indices):\n",
    "        text = [self.index_to_char[idx.item()] for idx in indices]\n",
    "        return \"\".join(text)\n",
    "\n",
    "\n",
    "corpus = \"piebfrhfbrfhchellols374t3842/,',v\" \n",
    "tokenizer = CharacterTokenizer(corpus)\n",
    "input_ids = tokenizer.encode(\"ello\")\n",
    "tokenizer.decode(input_ids)\n",
    "# It's good practice to specify dtype=torch.long when creating index tensors. \n",
    "# PyTorch's nn.Embedding layer, which you'd use next, expects its input to be of type LongTensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "10c394cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data loader \n",
    "# --- 1. The Dataset ---\n",
    "\n",
    "corpus = \"\"\"\n",
    "Friends, Romans, countrymen, lend me your ears;\n",
    "I come to bury Caesar, not to praise him.\n",
    "The evil that men do lives after them;\n",
    "The good is oft interred with their bones;\n",
    "So let it be with Caesar.\n",
    "\"\"\"\n",
    "# --- 2. Data Preparation ---\n",
    "tokenizer = CharacterTokenizer(corpus)\n",
    "data = tokenizer.encode(corpus)\n",
    "# Hyperparameters\n",
    "VOCAB_SIZE = tokenizer.vocab_size\n",
    "EMBED_SIZE = 32\n",
    "MAX_SEQ_LEN = 64\n",
    "NUM_HEADS = 4\n",
    "NUM_LAYERS = 3\n",
    "LEARNING_RATE = 1e-3\n",
    "TRAINING_STEPS = 20\n",
    "model = GPT(VOCAB_SIZE, EMBED_SIZE, MAX_SEQ_LEN, NUM_HEADS, NUM_LAYERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8f63da0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3.1098, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0527, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2153, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2053, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0135, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1087, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1025, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9334, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2142, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0939, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1510, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1032, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0563, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1462, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2239, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0631, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9085, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2700, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9816, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9221, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2798, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1600, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0454, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1840, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1793, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0023, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0341, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9819, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1521, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0558, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1022, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1022, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1534, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1496, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8979, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9935, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1315, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0415, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.2266, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0675, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1682, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8725, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9347, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9546, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0200, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1166, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8963, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.3465, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7814, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1052, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8753, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9296, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0090, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0039, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9836, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9569, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6478, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1876, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8526, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0080, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0746, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8530, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1218, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1190, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0266, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8287, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1007, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6695, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0780, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0810, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6655, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0068, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1542, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8718, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.0654, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8319, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1782, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9322, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1875, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8950, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8076, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7190, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8432, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7931, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9266, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8300, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9540, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8792, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9035, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9260, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9630, grad_fn=<NllLossBackward0>)\n",
      "tensor(3.1048, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7694, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.6024, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8836, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.9287, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.8924, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7090, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.5700, grad_fn=<NllLossBackward0>)\n",
      "tensor(2.7415, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "def get_batch(data, batch_size, block_size):\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
    "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
    "    return x ,y \n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr= LEARNING_RATE)\n",
    "loss = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# integrate the training loop with this \n",
    "for steps in range(100):\n",
    "    batch_data, batch_targets = get_batch(data, 4, 8)\n",
    "    # train data = (4, 8 ) \n",
    "    predictions, loss = model.forward(batch_data,batch_targets )\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "    # shape of predictions is same as batch_data \n",
    "    #(4, 8)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "fe26aaac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions shape: torch.Size([32, 64])\n",
      "Targets shape: torch.Size([32])\n",
      "Calculated Loss: 4.50061559677124\n"
     ]
    }
   ],
   "source": [
    "# just to show that the loss works with predictions batchs_size, ) and (batch_Size, probs ) tensors as well\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 1. Initialize the loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# 2. Create tensors with the exact shapes you described\n",
    "predictions = torch.randn(32, 64) # 32 items, 64 classes\n",
    "targets = torch.randint(0, 64, (32,)) # 32 correct answers (indices from 0 to 63)\n",
    "\n",
    "# 3. Calculate the loss\n",
    "loss = loss_fn(predictions, targets)\n",
    "\n",
    "print(f\"Predictions shape: {predictions.shape}\")\n",
    "print(f\"Targets shape: {targets.shape}\")\n",
    "print(f\"Calculated Loss: {loss.item()}\") # This will run without any errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0cefc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Iteration 1 ---\n",
      "Current splits: [['t', 'e', 's', 't'], ['b', 'e', 's', 't'], ['l', 'e', 's', 's', 'o', 'n']]\n",
      "Most frequent pair: ('e', 's') (count: 3)\n",
      "--- Iteration 2 ---\n",
      "Current splits: [['t', 'e', 's', 't'], ['b', 'e', 's', 't'], ['l', 'e', 's', 's', 'o', 'n']]\n",
      "Most frequent pair: ('es', 't') (count: 2)\n",
      "--- Iteration 3 ---\n",
      "Current splits: [['t', 'e', 's', 't'], ['b', 'e', 's', 't'], ['l', 'e', 's', 's', 'o', 'n']]\n",
      "Most frequent pair: ('t', 'est') (count: 1)\n",
      "--- Iteration 4 ---\n",
      "Current splits: [['t', 'e', 's', 't'], ['b', 'e', 's', 't'], ['l', 'e', 's', 's', 'o', 'n']]\n",
      "Most frequent pair: ('b', 'est') (count: 1)\n",
      "--- Iteration 5 ---\n",
      "Current splits: [['t', 'e', 's', 't'], ['b', 'e', 's', 't'], ['l', 'e', 's', 's', 'o', 'n']]\n",
      "Most frequent pair: ('l', 'es') (count: 1)\n",
      "--- Iteration 6 ---\n",
      "Current splits: [['t', 'e', 's', 't'], ['b', 'e', 's', 't'], ['l', 'e', 's', 's', 'o', 'n']]\n",
      "Most frequent pair: ('les', 's') (count: 1)\n",
      "--- Iteration 7 ---\n",
      "Current splits: [['t', 'e', 's', 't'], ['b', 'e', 's', 't'], ['l', 'e', 's', 's', 'o', 'n']]\n",
      "Most frequent pair: ('less', 'o') (count: 1)\n",
      "[['test'], ['best'], ['lesso', 'n']]\n"
     ]
    }
   ],
   "source": [
    "# byte pair encoding \n",
    "#BPE is a tokenization algorithm that creates a vocabulary of subwords (parts of words) instead of just\n",
    "#  individual characters. It starts with a vocabulary of single characters and iteratively merges the most \n",
    "# frequently occurring adjacent pairs of tokens into a new, single token.\n",
    "# Compared to character tokenizer, this is betetr because we increase the vocabulary size and decrease the seq length and this makes the trainign and infernece d\\fatser, \n",
    "from  collections import Counter\n",
    "initial_splits = [['t', 'e', 's', 't'], ['b', 'e', 's', 't'], ['l', 'e', 's', 's', 'o', 'n']]\n",
    "pair = ('e', 's')\n",
    "def merge_pair(initial_splits, pair):\n",
    "    new_word_splits = []\n",
    "    for word in initial_splits:\n",
    "        i = 0 \n",
    "        new_word = []\n",
    "        while i < len(word):\n",
    "            # check if we meet the condition \n",
    "            if i<len(word)-1 and (word[i], word[i+1]) == pair:\n",
    "                new_word.append(\"\".join([word[i], word[i+1]]))\n",
    "                i+=2\n",
    "            else:\n",
    "                new_word.append(word[i])\n",
    "                i+=1\n",
    "        new_word_splits.append(new_word)\n",
    "\n",
    "    return new_word_splits\n",
    "import collections# to do this iteratively we will get the best freq so far \n",
    "def get_stats(initial_splits):\n",
    "    c = collections.defaultdict(int)\n",
    "    for word in initial_splits:\n",
    "        i = 0 \n",
    "        while i<len(word)-1:\n",
    "            pair = (word[i], word[i+1])\n",
    "            c[pair] +=1\n",
    "            i+=1\n",
    "    #sorted_pairs = sorted(c.items(), key = lambda x:x[1], reverse = True)\n",
    "    return c\n",
    "splits = initial_splits\n",
    "for i in range(7):\n",
    "    print(f\"--- Iteration {i+1} ---\")\n",
    "    print(f\"Current splits: {splits}\")\n",
    "    pair_stats = get_stats(initial_splits)\n",
    "    best_pair = max(pair_stats, key=pair_stats.get)\n",
    "    initial_splits = merge_pair(initial_splits,best_pair)\n",
    "    print(f\"Most frequent pair: {best_pair} (count: {pair_stats[best_pair]})\")\n",
    "    \n",
    "print(initial_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ab4d6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cdde62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
