{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "71fb4b7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch \n",
    "query = torch.tensor([1.0, 0.0, 0.0])\n",
    "key = torch.tensor([1.0, 0.0, 1.0])\n",
    "query.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "70a4837c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.5774)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.5774)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_k = key.shape[-1]                   # this is an int\n",
    "import math \n",
    "scale = math.sqrt(d_k)   \n",
    "attention_score = torch.matmul(query, key.T)/scale\n",
    "print(attention_score)\n",
    "attention_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1dc7d0a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Causal atttention block implementation \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "class CausalAttentionBlock(torch.nn.Module):\n",
    "    def __init__(self, d_model, n_heads, block_size, dropout = 0.1):\n",
    "        super().__init__() # whta does this do ?? \n",
    "        assert d_model % n_heads == 0, \"d_model must be divisible by n_head\"\n",
    "        self.d_model = d_model \n",
    "        self.n_heads = n_heads\n",
    "        self.d_head = d_model // n_heads\n",
    "        self.query = nn.Linear(d_model, d_model)\n",
    "        self.key = nn.Linear(d_model, d_model)\n",
    "        self.value = nn.Linear(d_model, d_model)\n",
    "        self.projection =  nn.Linear(d_model, d_model, bias=False)\n",
    "        self.attn_dropout = nn.Dropout(dropout)\n",
    "        # block_size is the max seq_length here \n",
    "        self.mask = torch.tril(torch.ones(block_size, block_size))\n",
    "        self.register_buffer(\"mask\", self.mask.view(1, 1, block_size, block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, seq_len, d_model = x.shape\n",
    "        # now we can split this tp head \n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "        v = self.value(x)\n",
    "        # get the head split and transpose \n",
    "        q = q.view(batch, seq_len, self.n_heads, self.d_head).transpose(1, 2) \n",
    "        v = v.view(batch, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
    "        k = k.view(batch, seq_len, self.n_heads, self.d_head).transpose(1, 2)\n",
    "\n",
    "        # now we can compute the attention for all these heads since last two matrices are gettign multiplied \n",
    "        attention_score = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_head) # batch , head, seq_len, seq_len\n",
    "        # now we need to mask this \n",
    "        attention_score = attention_score.masked_fill(self.mask[:, :, :seq_len, :seq_len] == 0, float('-inf'))\n",
    "         \n",
    "        attention_score = torch.softmax(attention_score, dim=-1)\n",
    "        attn = self.attn_dropout(attention_score)  #\n",
    "        # now we can compute the attention\n",
    "        out = torch.matmul(attn, v) #(B, H, T, d_head)\n",
    "        out = out.transpose(1,2).contiguous().view(batch, seq_len, d_model)\n",
    "        out = self.projection(out) \n",
    "        return out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5fb09018",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 3, 4])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### RMS NORM \n",
    "x = torch.randn(2, 3, 4)\n",
    "print(x.shape)\n",
    "class RMSNorm(nn.Module):\n",
    "    def __init__(self, epsilon= 1e-15):\n",
    "        super().__init__()\n",
    "        self.epsilon = epsilon \n",
    "        self.scale = nn.Parameter(torch.ones(1))\n",
    "    def forward(self,x):\n",
    "        norm = torch.sqrt(torch.mean(x**2, dim=-1, keepdim=True) + self.epsilon)\n",
    "        return (x / norm) * self.scale\n",
    "rms = RMSNorm()\n",
    "y = rms(x)\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf120b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "220b7bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mutli head attention \n",
    "def multihead_attention(q,k,v, d_model, n_heads, mask=None):\n",
    "    \"\"\"\n",
    "    Implements multi-head attention.\n",
    "    \n",
    "    Args:\n",
    "        q (Tensor): Query tensor of shape (batch_size, seq_len, d_model)\n",
    "        k (Tensor): Key tensor of shape (batch_size, seq_len, d_model)\n",
    "        v (Tensor): Value tensor of shape (batch_size, seq_len, d_model)\n",
    "        num_heads (int): Number of attention heads\n",
    "        d_model (int): Total embedding dimension\n",
    "        mask (Tensor, optional): Masking tensor for attention\n",
    "    Returns:\n",
    "        Tensor: Multi-head attention output of shape (batch_size, seq_len, d_model)\n",
    "    \"\"\"\n",
    "    d_head = d_model // n_heads\n",
    "    device = q.device\n",
    "    batch_size, seq_len, _ = q.shape\n",
    "    Q = nn.Linear(d_model, d_model , bias=False).to(device )\n",
    "    K = nn.Linear(d_model, d_model, bias=False).to(device )\n",
    "    V=  nn.Linear(d_model, d_model, bias=False).to(device )\n",
    "    W_out = nn.Linear(d_model, d_model, bias=False).to(device)\n",
    "\n",
    "    # now we get the projections for q,k,v for each head \n",
    "    q = Q(q)\n",
    "    k = K(k)\n",
    "    v = V(v)\n",
    "\n",
    "    q = q.view(batch_size, seq_len, n_heads, d_head).transpose(1, 2)\n",
    "    k = k.view(batch_size, seq_len, n_heads, d_head).transpose(1, 2)\n",
    "    v = v.view(batch_size, seq_len, n_heads, d_head).transpose(1, 2)\n",
    "\n",
    "    # now we can compute the attention for all these heads since last two matrices are gettign multiplied\n",
    "    attention_score = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_head) # batch , head, seq_len, seq_len\n",
    "    if mask is not None:\n",
    "        # assume mask is a tensor of shape (batch_size, seq_len, seq_len)\n",
    "        attention_score = attention_score.masked_fill(mask == 0, float('-inf'))\n",
    "    attention_score = torch.softmax(attention_score, dim=-1)\n",
    "    # nw mutliply by V \n",
    "    output = torch.matmul(attention_score, v) # (batch_size, n_heads, seq_len, d_head)\n",
    "    output = output.transpose(1,2).contiguous().view(batch_size, seq_len, d_model)\n",
    "    return W_out(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c6908cfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 8])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "batch_size = 3\n",
    "seq_len = 4\n",
    "d_model = 8\n",
    "n_heads = 2\n",
    "\n",
    "q = torch.rand(batch_size, seq_len, d_model)\n",
    "k = torch.rand(batch_size, seq_len, d_model)\n",
    "v = torch.rand(batch_size, seq_len, d_model)\n",
    "print(q.shape)\n",
    "\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\"\n",
    "output_custom = multihead_attention(q, k, v, d_model, n_heads)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cd47bb2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 4, 8])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_custom.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "061d51f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "multihead_attn = torch.nn.MultiheadAttention(embed_dim=d_model, num_heads=n_heads, bias=False, batch_first=True)\n",
    "output, _ = multihead_attn(q, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "298b6a51",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAssertionError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m torch.allclose(output_custom, output, atol=\u001b[32m1e-08\u001b[39m, rtol=\u001b[32m1e-05\u001b[39m) \u001b[38;5;66;03m# Check if they are close enough.\u001b[39;00m\n",
      "\u001b[31mAssertionError\u001b[39m: "
     ]
    }
   ],
   "source": [
    "assert torch.allclose(output_custom, output, atol=1e-08, rtol=1e-05) # Check if they are close enough.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "37a0f120",
   "metadata": {},
   "outputs": [],
   "source": [
    "###########Sinusoidal PE ##############\n",
    "class SinusoidalPositionalEmbedding(nn.Module):\n",
    "    def __init__(self,max_seq_len, d_model):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.max_seq_len = max_seq_len\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        pos = torch.arange(0, max_seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(pe)\n",
    "        # this is very annoying we will do this later "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8858c3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 32])\n"
     ]
    }
   ],
   "source": [
    "# lets do grouped query attention, \n",
    "# instead of the grouping by heads with equal number of q,k,v matrices, \n",
    "# split the number of queries in groups\n",
    "\n",
    "torch.manual_seed(42)\n",
    "batch_size = 3\n",
    "seq_len = 4\n",
    "d_model = 32\n",
    "num_heads = 2\n",
    "\n",
    "q = torch.rand(batch_size, seq_len, d_model)\n",
    "k = torch.rand(batch_size, seq_len, d_model)\n",
    "v = torch.rand(batch_size, seq_len, d_model)\n",
    "print(q.shape)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ea24e077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 32])\n",
      "torch.Size([3, 4, 16])\n",
      "torch.Size([3, 8, 4, 4])\n",
      "torch.Size([3, 8, 4, 4])\n",
      "torch.Size([3, 8, 4, 4])\n",
      "torch.Size([3, 8, 4, 4])\n",
      "torch.Size([3, 4, 8, 4])\n",
      "torch.Size([3, 4, 32])\n",
      "torch.Size([3, 4, 32])\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "num_heads = 8\n",
    "batch_size = 3\n",
    "seq_len = 4\n",
    "d_model = 32\n",
    "\n",
    "q = torch.rand(batch_size, seq_len, d_model)\n",
    "k = torch.rand(batch_size, seq_len, d_model)\n",
    "v = torch.rand(batch_size, seq_len, d_model)\n",
    "num_query_groups = 4 \n",
    "#def grouped_query_attention(q, k, v, num_query_groups, d_model, mask=None):\n",
    "# so given qk v and q_groups, we can calculate the attention scores \n",
    "# and then we can split the q and v into num_query_groups\n",
    "d_head = d_model // num_heads\n",
    "batch_size, seq_len, d_model = q.shape\n",
    "Q = nn.Linear(d_model, d_model, bias=False).to(device )\n",
    "K = nn.Linear(d_model,  num_query_groups * d_head, bias=False).to(device )\n",
    "V = nn.Linear(d_model,  num_query_groups * d_head, bias=False).to(device ) \n",
    "W_out = nn.Linear(d_model, d_model, bias=False).to(device)\n",
    "# Q operates over all heads so its final dimension is d_model \n",
    "# KV operates over all the d_head its shared across head??\n",
    "\n",
    "q = Q(q)\n",
    "k = K(k)\n",
    "v = V(v)\n",
    "print(q.shape)\n",
    "print(k.shape)\n",
    "# now we can split the q into q groups \n",
    "# last dimenaion is d_model , we split that into num_query_groups\n",
    "q = q.view(batch_size, seq_len, num_heads, d_head) # [batch_size, seq_len, num_query_groups, d_head]\n",
    "# now we have to do \n",
    "v = v.view(batch_size, seq_len, num_query_groups, d_head) # [batch_size, seq_len, num_query_groups, d_head]\n",
    "k = k.view(batch_size, seq_len, num_query_groups, d_head) # [batch_size, seq_len, num_query_groups, d_head]\n",
    "\n",
    "# now we can prepare for scaled dot product, first transpose \n",
    "q = q.transpose(1,2)\n",
    "k = k.transpose(1,2)\n",
    "v = v.transpose(1,2)\n",
    "# no of repititions = no of heads/ no of query groups\n",
    "# now repeat kv fro each head \n",
    "k = k.repeat_interleave(num_heads//num_query_groups, dim=1)\n",
    "v = v.repeat_interleave(num_heads//num_query_groups, dim= 1)\n",
    "print(k.shape)\n",
    "# so the shapes match now \n",
    "print(q.shape)\n",
    "\n",
    "# great \n",
    "# now we can compute the attention for all these heads since last two matrices are gettign multiplied\n",
    "attention_score = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(d_head) # batch , head, seq_len, seq_len\n",
    "print(attention_score.shape)\n",
    "attention_score = torch.softmax(attention_score, dim=-1)\n",
    "attention_score = torch.matmul(attention_score, v) # (batch_size, n_heads, seq_len, d_head)\n",
    "print(attention_score.shape)\n",
    "# now we need to get it back to batch, seq, d_model dimension and apply W_o \n",
    "\n",
    "attention_score = attention_score.transpose(1,2).contiguous()\n",
    "print(attention_score.shape)\n",
    "# and combine the last two dimension \n",
    "attention_score = attention_score.view(batch_size, seq_len, d_model)\n",
    "print(attention_score.shape)\n",
    "output = W_out(attention_score)\n",
    "print(output.shape  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d27696bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 5])\n"
     ]
    }
   ],
   "source": [
    "# Create Embeddings out of an LLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from datasets import load_dataset\n",
    "#dataset = load_dataset(\"gmongaras/Amazon-Reviews-2023\", trust_remote_code=True)\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"HuggingFaceTB/SmolLM2-135M\")\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "text = \"hello how are you \"\n",
    "texts = [text] * 10\n",
    "encodings = tokenizer(texts, return_tensors=\"pt\", padding=False, truncation=True)\n",
    "print(encodings[\"input_ids\"].shape)\n",
    "input_ids = encodings['input_ids'].to(device)\n",
    "attention_mask = encodings['attention_mask'].to(device)\n",
    "\n",
    "# 4. Forward pass with output_hidden_states=True to get all hidden states\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, output_hidden_states=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00209c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size torch.Size([10, 5, 49152])\n",
      "torch.Size([10, 5, 576])\n"
     ]
    }
   ],
   "source": [
    "print(\"vocab size\",outputs.logits.shape)\n",
    "last_hidden_states = outputs.hidden_states[-1]\n",
    "print(last_hidden_states.shape)\n",
    "# last hidden state gives us the token embedding for each token in the sequence\n",
    "# we can use this to compute sentence embeddings by averaging token embeddings excluding padding tokens\n",
    "sentence_embeddings = torch.mean(last_hidden_states * attention_mask.unsqueeze(-1), dim=1)\n",
    "expanded_mask = attention_mask.unsqueeze(-1).expand(last_hidden_states.size()).float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5483556b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence embeddings shape: torch.Size([10, 576])\n"
     ]
    }
   ],
   "source": [
    "sum_embeddings = torch.sum(last_hidden_states * expanded_mask, dim=1)\n",
    "sum_mask = torch.clamp(expanded_mask.sum(dim=1), min=1e-9)  # avoid division by zero\n",
    "sentence_embeddings = sum_embeddings / sum_mask  # (batch_size, hidden_dim)\n",
    "print(\"Sentence embeddings shape:\", sentence_embeddings.shape)  # (10, hidden_dim)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbefeee7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0de35bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f4f068",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a38039c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
