{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9130afa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 3.9811e-01, 1.5849e-01, 6.3096e-02, 2.5119e-02, 1.0000e-02,\n",
       "        3.9811e-03, 1.5849e-03, 6.3096e-04, 2.5119e-04])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Rotary position embedding \n",
    "# instead of calculating the pe, rotate the key and query vectors \n",
    "#This is done by splitting the embedding dimension into pairs, treating each pair as a complex number, and multiplying it by a rotation factor e imθ\n",
    "#The angle of rotation depends on the token's position m. \n",
    "# this is done to handle the relative positions in the  embeddings, so that the context length can be increased more than the max seq length \n",
    "import torch \n",
    "dim = 20\n",
    "inv_freq = 1.0 / (10000 ** (torch.arange(0, dim, 2).float() / dim))\n",
    "inv_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "050afce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If fullfinetuing layer number of parameters:  33024\n",
      "For Lora layer number of parameters:  1796\n"
     ]
    }
   ],
   "source": [
    "# Parameter-Efficient Fine-Tuning (LoRA)\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "in_features = 128 \n",
    "out_features = 256 \n",
    "original_linear = nn.Linear(in_features=128, out_features=256)\n",
    "rank = 4 \n",
    "alpha = 8 \n",
    "scale  = alpha/rank\n",
    "# we define two layers for A and B\n",
    "A = nn.Linear(in_features, rank)\n",
    "B = nn.Linear(rank, out_features)\n",
    "# B has zero weights \n",
    "nn.init.zeros_(B.weight)\n",
    "x = torch.randn(1,128)\n",
    "output = original_linear(x) + B(A(x))*scale\n",
    "# full finetuning layer weights \n",
    "finetuning_parameters = 0 \n",
    "for x in original_linear.parameters():\n",
    "    finetuning_parameters+= x.numel()\n",
    "print(\"If fullfinetuing layer number of parameters: \", finetuning_parameters)\n",
    "new_param =0 \n",
    "for x in B.parameters():\n",
    "    new_param+= x.numel()\n",
    "for x in A.parameters():\n",
    "    new_param+= x.numel()\n",
    "print(\"For Lora layer number of parameters: \", new_param)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "db08fd61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])\n",
      "Mask shape: torch.Size([1, 1, 10, 10])\n",
      "torch.Size([4, 10, 20])\n"
     ]
    }
   ],
   "source": [
    "# Fast Attention \n",
    "# imporve the for loops in the Multihrad attention and imporve the for loops in the n_layers \n",
    "# A for loop in the forward pass is a performance bottleneck because it processes each head sequentially instead of in parallel on the GPU.\n",
    "import torch \n",
    "import math \n",
    "def scaled_dot_product_attention(Q, K, V, mask ):\n",
    "    # the shape of QKV is batch_size, num_heads, seq_len, dk \n",
    "    dot_product = torch.matmul(Q, K.transpose(-1, -2)) \n",
    "    # dot product shape is batch_size, num_heads, seq_len, seq_len\n",
    "    dk = Q.shape[-1]\n",
    "    scaled_dot_product = dot_product/ math.sqrt(dk)\n",
    "    \n",
    "    if mask is not None:\n",
    "        scaled_dot_product = torch.masked_fill(scaled_dot_product,mask==0, value=float(\"-inf\"))\n",
    "    scaled_dot_product = torch.softmax(scaled_dot_product, dim=-1)\n",
    "    attention_scores = torch.matmul(scaled_dot_product, V)\n",
    "    return attention_scores, scaled_dot_product\n",
    "\n",
    "class FastMultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, num_heads, embed_size):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads \n",
    "        self.embed_size = embed_size \n",
    "        self.dk = embed_size// num_heads\n",
    "        # for parallel ocmputaion we create a. single layer of qkv matrices instead of 3 \n",
    "        self.qkv = nn.Linear(embed_size, 3*embed_size)# we usually dont have a bias here \n",
    "        self.output_layer = nn.Linear(embed_size, embed_size)\n",
    "    def forward(self,x, mask):\n",
    "        # x shape is batch_size, seq_len, embed_size\n",
    "        qkv = self.qkv(x) \n",
    "        batch_size = x.shape[0]\n",
    "        # so this computation is getting parallely executed on gpu \n",
    "        # earlier we used to do q = nn.linear(x) v = nn.linear(x) whihc brings the series computations\n",
    "        # qkv will be  batch_size, seq_len, 3*embed_size \n",
    "        Q, K , V = torch.chunk(qkv, chunks=3, dim = -1)\n",
    "        # now we can proceed wiht normal scaled dot product attention \n",
    "        # make parallel across heads \n",
    "        Q = Q.view(batch_size, -1,self.num_heads , self.dk ).transpose(1,2)\n",
    "        V = V.view(batch_size, -1,self.num_heads , self.dk ).transpose(1,2)\n",
    "        K = K.view(batch_size, -1,self.num_heads , self.dk ).transpose(1,2)\n",
    "\n",
    "        attention_output,_ = scaled_dot_product_attention(Q, K, V, mask)\n",
    "        # shape is batch_size, num_heads, seq_len, dk \n",
    "        attention_output = attention_output.transpose(1,2)\n",
    "        attention_output = attention_output.contiguous().view(batch_size,-1, self.embed_size )\n",
    "        # we need conitguous because the transpose makes the memory non contigous??\n",
    "        output = self.output_layer(attention_output)\n",
    "        \n",
    "        return output\n",
    "batch_size = 4; seq_len = 10 ; embedding_dim = 20\n",
    "model = FastMultiHeadAttention(num_heads= 4, embed_size=embedding_dim)\n",
    "input_tensor = torch.rand(batch_size, seq_len, embedding_dim)\n",
    "mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(0)\n",
    "print(mask)\n",
    "print(f\"Mask shape: {mask.shape}\")\n",
    "output = model(input_tensor, mask=mask)\n",
    "print(output.shape)\n",
    "#decoder = DecoderOnlyBlock(embedding_dim, num_heads, 4*embedding_dim, 0.9)\n",
    "# batch_size = 4; seq_len = 10 ; embedding_dim = 20; num_heads = 4 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "9b7d1367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 0., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 1., 0., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 0., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 0., 0., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 0.],\n",
      "          [1., 1., 1., 1., 1., 1., 1., 1., 1., 1.]]]])\n",
      "Mask shape: torch.Size([1, 1, 10, 10])\n",
      "torch.Size([4, 10, 20])\n"
     ]
    }
   ],
   "source": [
    "# we can make the decoder only attention block usign this attention module \n",
    "class DecoderOnlyBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, num_heads, dff, dropout):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dff = dff \n",
    "        self.dropout = dropout \n",
    "        self.ffn = nn.Sequential(nn.Linear(embedding_dim, dff), nn.ReLU(), nn.Linear(dff,embedding_dim))\n",
    "        self.attention = FastMultiHeadAttention(num_heads= 4, embed_size=embedding_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.layernorm1 = nn.LayerNorm(embedding_dim)\n",
    "        self.layernorm2 = nn.LayerNorm(embedding_dim)\n",
    "    def forward(self , x, mask ):\n",
    "        # assuming x is hte embedding here, batch_size, seq_len, embed_dim \n",
    "        # mask is 1, 1,, seq_len, seq_len\n",
    "        residual = x \n",
    "        attention_output = self.attention(x, mask)\n",
    "        \n",
    "        x = residual+ self.layernorm1(attention_output)\n",
    "        residual = x \n",
    "        FFN_output = self.ffn(x)\n",
    "        FFN_output = self.layernorm2(FFN_output)\n",
    "        x = residual + self.dropout(FFN_output)\n",
    "        return x \n",
    "batch_size = 4; seq_len = 10 ; embedding_dim = 20\n",
    "input_tensor = torch.rand(batch_size, seq_len, embedding_dim)\n",
    "mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(0)\n",
    "print(mask)\n",
    "num_heads = 4 \n",
    "print(f\"Mask shape: {mask.shape}\")\n",
    "decoder = DecoderOnlyBlock(embedding_dim,num_heads , 4*embedding_dim, 0.9)\n",
    "batch_size = 4; seq_len = 10 ; embedding_dim = 20; num_heads = 4 \n",
    "attention_block_output = decoder(input_tensor, mask )\n",
    "print(attention_block_output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "6bcba68b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10])\n",
      "torch.Size([4, 30])\n"
     ]
    }
   ],
   "source": [
    "# make the full gpt module now \n",
    "class GPT(nn.Module): \n",
    "    def __init__(self, embed_size, max_seq_len, vocab_size, num_heads, num_layers ):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # nn.Embedding needs a int 64 inputs Because nn.Embedding is a lookup table, not a linear layer.\n",
    "        self.positional_embedding = nn.Embedding(self.max_seq_len, self.embed_size) #lets make this learnable as well\n",
    "        self.attention_blocks = nn.ModuleList([DecoderOnlyBlock(embedding_dim=embed_size, num_heads=num_heads, \n",
    "                                                                dff = 4*embed_size, dropout=0.9) \n",
    "                                                                for _ in range(self.num_layers)])\n",
    "        self.lm_head  = nn.Linear(self.embed_size, self.vocab_size)\n",
    "        self.layernorm = nn.LayerNorm(self.embed_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # expect x to eb the pretraining data wiht batch size, seq_len \n",
    "        batch_size, seq_len = x.shape\n",
    "        # call the embedding and ocmibine with positional embedding \n",
    "        token_embedding = self.embedding(x) # batch_size, seq_len, embed_size\n",
    "    \n",
    "        pos_indices = torch.arange(seq_len)\n",
    "        positional_embedding = self.positional_embedding(pos_indices)\n",
    "        x = token_embedding+ positional_embedding\n",
    "        \n",
    "        # shape batch_size, seq_len,embed_size\n",
    "        # now we can pass this through the attention modules with the mask \n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(0)\n",
    "         \n",
    "        # 1, 1, max_seq, max_seq\n",
    "        # make the causal maask \n",
    "        for layer in self.attention_blocks:\n",
    "            x = layer(x, mask)\n",
    "            # x same as output batch_size, max_seq, embed_size\n",
    "        output = self.layernorm(x)\n",
    "        # otuput shape batch, seq\n",
    "        y = self.lm_head(output)\n",
    "        return y # here y is the last token prob\n",
    "    # lets add a generate call for this \n",
    "    # this is the decoding step which calls the froward and samples the token from this recursively \n",
    "    # this is the inference code \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature = 1.0):\n",
    "        self.eval()\n",
    "        #idx shape is previous tokens processsed so (batch_size x seq_len )\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.max_seq_len:]\n",
    "            logits = self.forward(idx_cond) # batch_size, seq_len, vocab_size\n",
    "            \n",
    "            # get logits from the last time step last seq token basically \n",
    "            last_token_logits = logits[:, -1, :] # b , vocab size \n",
    "            \n",
    "            last_token_logits = last_token_logits / temperature\n",
    "            last_token_probs = torch.softmax(last_token_logits, dim= -1) # the softmax is over all the vocab \n",
    "            # sampple the next token torch.multinomial samples an index based on the weights (probabilities)\n",
    "            last_token = torch.multinomial(last_token_probs, num_samples=1) # B x1 \n",
    "            # print(\"logits\", logits.shape)\n",
    "            # print(last_token.shape)\n",
    "            idx = torch.cat((idx, last_token), dim = 1)\n",
    "            # B, S+1\n",
    "        return idx\n",
    "vocab_size = 1000\n",
    "embedding_dim = 20\n",
    "max_seq_len = 100\n",
    "num_heads = 4\n",
    "num_layers = 6 \n",
    "batch_size = 4; seq_len = 10 ; embedding_dim = 20\n",
    "model = GPT(embed_size= embedding_dim, max_seq_len=max_seq_len, vocab_size=vocab_size,num_heads=num_heads,num_layers = num_layers)\n",
    "embed_size = embedding_dim\n",
    "input_tensor = torch.randint(0, vocab_size, (batch_size,10)) # (batch, seq_len)\n",
    "# # pass the input as ranint rather than rand \n",
    "output = model.generate(input_tensor, 20 )\n",
    "print(input_tensor.shape)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195ca456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10])\n",
      "tensor(6.9699, grad_fn=<NllLossBackward0>)\n",
      "torch.Size([4, 13])\n"
     ]
    }
   ],
   "source": [
    "# add the loss to calculate cross entropy in this case \n",
    "class GPT(nn.Module): \n",
    "    def __init__(self, embed_size, max_seq_len, vocab_size, num_heads, num_layers ):\n",
    "        super().__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_heads = num_heads\n",
    "        self.num_layers = num_layers \n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        # nn.Embedding needs a int 64 inputs Because nn.Embedding is a lookup table, not a linear layer.\n",
    "        self.positional_embedding = nn.Embedding(self.max_seq_len, self.embed_size) #lets make this learnable as well\n",
    "        self.attention_blocks = nn.ModuleList([DecoderOnlyBlock(embedding_dim=embed_size, num_heads=num_heads, \n",
    "                                                                dff = 4*embed_size, dropout=0.9) \n",
    "                                                                for _ in range(self.num_layers)])\n",
    "        self.lm_head  = nn.Linear(self.embed_size, self.vocab_size)\n",
    "        self.layernorm = nn.LayerNorm(self.embed_size)\n",
    "        \n",
    "    def forward(self, x, targets=None):\n",
    "        ## FORWARD SHOULD RETURN THE LOSS HERE SO THAT WE CAN DO BACKPROP \n",
    "        # expect x to eb the pretraining data wiht batch size, seq_len \n",
    "        batch_size, seq_len = x.shape\n",
    "        # call the embedding and ocmibine with positional embedding \n",
    "        token_embedding = self.embedding(x) # batch_size, seq_len, embed_size\n",
    "    \n",
    "        pos_indices = torch.arange(seq_len)\n",
    "        positional_embedding = self.positional_embedding(pos_indices)\n",
    "        x = token_embedding+ positional_embedding\n",
    "        \n",
    "        # shape batch_size, seq_len,embed_size\n",
    "        # now we can pass this through the attention modules with the mask \n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len)).unsqueeze(0).unsqueeze(0)\n",
    "         \n",
    "        # 1, 1, max_seq, max_seq\n",
    "        # make the causal maask \n",
    "        for layer in self.attention_blocks:\n",
    "            x = layer(x, mask)\n",
    "            # x same as output batch_size, max_seq, embed_size\n",
    "        output = self.layernorm(x)\n",
    "        # otuput shape batch, seq\n",
    "        logits = self.lm_head(output)\n",
    "\n",
    "        if targets is None: \n",
    "            ce_loss = None\n",
    "        else:\n",
    "            # lets say the targets are next tokens ie the tokens shifted by 1 \n",
    "            # batch, seq_len \n",
    "            # and we get the logits as batch, seq_len, vocab \n",
    "            targets = targets.view(-1)\n",
    "            predictions = logits.view(-1,vocab_size) \n",
    "            loss = nn.CrossEntropyLoss()\n",
    "            ce_loss = loss(predictions, targets)\n",
    "            # this goes in the backward loss.backward()\n",
    "\n",
    "        return logits,ce_loss# here y is the last token prob\n",
    "    # lets add a generate call for this \n",
    "    # this is the decoding step which calls the froward and samples the token from this recursively \n",
    "    # this is the inference code \n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature = 1.0):\n",
    "        self.eval()\n",
    "        #idx shape is previous tokens processsed so (batch_size x seq_len )\n",
    "        for _ in range(max_new_tokens):\n",
    "            idx_cond = idx[:, -self.max_seq_len:]\n",
    "            logits ,_ = self.forward(idx_cond) # batch_size, seq_len, vocab_size\n",
    "            \n",
    "            # get logits from the last time step last seq token basically \n",
    "            last_token_logits = logits[:, -1, :] # b , vocab size \n",
    "            \n",
    "            last_token_logits = last_token_logits / temperature\n",
    "            last_token_probs = torch.softmax(last_token_logits, dim= -1) # the softmax is over all the vocab \n",
    "            # sampple the next token torch.multinomial samples an index based on the weights (probabilities)\n",
    "            last_token = torch.multinomial(last_token_probs, num_samples=1) # B x1 \n",
    "            # print(\"logits\", logits.shape)\n",
    "            # print(last_token.shape)\n",
    "            idx = torch.cat((idx, last_token), dim = 1)\n",
    "            # B, S+1\n",
    "        return idx\n",
    "vocab_size = 1000\n",
    "embed_size = 64\n",
    "max_seq_len = 128\n",
    "num_heads = 4\n",
    "num_layers = 4\n",
    "batch_size = 4; seq_len = 10 ; embedding_dim = 20\n",
    "model = GPT(embed_size, max_seq_len, vocab_size,num_heads,num_layers)\n",
    "\n",
    "input_tensor = torch.randint(0, vocab_size, (batch_size,seq_len)) # (batch, seq_len)\n",
    "targets = torch.randint(0, vocab_size, (batch_size, seq_len)) # The \"correct\" next tokens\n",
    "# # pass the input as ranint rather than rand \n",
    "output = model.forward(input_tensor, targets )\n",
    "print(input_tensor.shape)\n",
    "\n",
    "print(output[1])\n",
    "generated_seq = model.generate(input_tensor, 3)\n",
    "print(generated_seq.shape)  #batch size x seq len "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "04f35fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30.8835, grad_fn=<NllLossBackward0>)\n",
      "tensor(30.8662, grad_fn=<NllLossBackward0>)\n",
      "tensor(30.8490, grad_fn=<NllLossBackward0>)\n",
      "tensor(30.8318, grad_fn=<NllLossBackward0>)\n",
      "tensor(30.8146, grad_fn=<NllLossBackward0>)\n",
      "tensor(30.7975, grad_fn=<NllLossBackward0>)\n",
      "tensor(30.7803, grad_fn=<NllLossBackward0>)\n",
      "tensor(30.7632, grad_fn=<NllLossBackward0>)\n",
      "tensor(30.7462, grad_fn=<NllLossBackward0>)\n",
      "tensor(30.7291, grad_fn=<NllLossBackward0>)\n",
      "tensor(30.7121, grad_fn=<NllLossBackward0>)\n",
      "tensor(30.6951, grad_fn=<NllLossBackward0>)\n",
      "tensor(30.6781, grad_fn=<NllLossBackward0>)\n",
      "tensor(30.6612, grad_fn=<NllLossBackward0>)\n",
      "tensor(30.6443, grad_fn=<NllLossBackward0>)\n",
      "tensor(30.6274, grad_fn=<NllLossBackward0>)\n",
      "tensor(30.6105, grad_fn=<NllLossBackward0>)\n",
      "tensor(30.5937, grad_fn=<NllLossBackward0>)\n",
      "tensor(30.5769, grad_fn=<NllLossBackward0>)\n",
      "tensor(30.5601, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=.0001)\n",
    "import torch\n",
    "for _ in range(20): # these are 10 steps\n",
    "    \n",
    "    target_layer = model.attention_blocks[0].ffn[0].weight\n",
    "    weights_before = target_layer.clone().detach()\n",
    "\n",
    "    # --- Your Standard Training Step ---\n",
    "    optimizer.zero_grad()\n",
    "    logits, loss = model(input_tensor, targets=targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(loss)\n",
    "\n",
    "    # weights_after = target_layer\n",
    "\n",
    "    # # 4. Compare a slice of the weights\n",
    "    # print(\"--- Weight Inspection ---\")\n",
    "    # print(f\"Loss: {loss.item():.4f}\\n\")\n",
    "    # print(f\"Slice of weights BEFORE update:\\n{weights_before[0, :5]}\\n\")\n",
    "    # print(f\"Slice of weights AFTER update:\\n{weights_after[0, :5]}\\n\")\n",
    "\n",
    "    # # 5. Verify that a change occurred\n",
    "    # difference = torch.sum(torch.abs(weights_before - weights_after))\n",
    "    # print(f\"Sum of absolute difference: {difference.item()}\")\n",
    "    # if difference > 0:\n",
    "    #     print(\"\\n✅ The weights have been successfully updated!\")\n",
    "    # else:\n",
    "    #     print(\"\\n❌ The weights did NOT change.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae3fc74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ello'"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Character tokenizer \n",
    "import torch \n",
    "# This is not a torch.nn.Module because it doesn't have any learnable parameters.\n",
    "class CharacterTokenizer: \n",
    "    def __init__(self, corpus):\n",
    "        # find all the unique characters in the corpus \n",
    "        # map them to a dictionary \n",
    "        # reverse map it to the tokens \n",
    "        self.vocab = sorted(list(set(corpus.strip())))\n",
    "        self.vocab_size = len(self.vocab)\n",
    "        self.char_to_index = {char:i for i,char in enumerate(self.vocab)}\n",
    "        self.index_to_char  = {i:char for i,char in enumerate(self.vocab)}\n",
    "    def encode(self, text):\n",
    "        indices = [self.char_to_index[char] for char in text]\n",
    "        return torch.tensor(indices)\n",
    "    def decode(self, indices):\n",
    "        text = [self.index_to_char[idx.item()] for idx in indices]\n",
    "        return \"\".join(text)\n",
    "\n",
    "\n",
    "corpus = \"piebfrhfbrfhchellols374t3842/,',v\" \n",
    "tokenizer = CharacterTokenizer(corpus)\n",
    "input_ids = tokenizer.encode(\"ello\")\n",
    "tokenizer.decode(input_ids)\n",
    "# It's good practice to specify dtype=torch.long when creating index tensors. \n",
    "# PyTorch's nn.Embedding layer, which you'd use next, expects its input to be of type LongTensor.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "776db59f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558e6f14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5444486f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
